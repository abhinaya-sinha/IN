{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require(['codemirror/mode/clike/clike'], function(Clike) { console.log('ROOTaaS - C++ CodeMirror module loaded'); });"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "IPython.CodeCell.config_defaults.highlight_modes['magic_text/x-c++src'] = {'reg':[/^%%cpp/]};"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to ROOTaaS 6.06/04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in <TUnixSystem::FindDynamicLibrary>: libDelphes[.so | .dll | .dylib | .sl | .dl | .a] does not exist in /usr/local/root/lib:/home/avikar/promc/promc/lib:/home/avikar/promc/promc/lib:/usr/local/cuda/targets/x86_64-linux/lib/:.:/usr/local/root/lib:/lib/x86_64-linux-gnu/tls/x86_64:/lib/x86_64-linux-gnu/tls:/lib/x86_64-linux-gnu/x86_64:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/tls/x86_64:/usr/lib/x86_64-linux-gnu/tls:/usr/lib/x86_64-linux-gnu/x86_64:/usr/lib/x86_64-linux-gnu:/lib/tls/x86_64:/lib/tls:/lib/x86_64:/lib:/usr/lib/tls/x86_64:/usr/lib/tls:/usr/lib/x86_64:/usr/lib\r\n",
      "input_line_34:1:10: fatal error: 'classes/DelphesClasses.h' file not found\r\n",
      "#include \"classes/DelphesClasses.h\"\r\n",
      "         ^\r\n",
      "input_line_35:1:10: fatal error: 'external/ExRootAnalysis/ExRootTreeReader.h' file not found\r\n",
      "#include \"external/ExRootAnalysis/ExRootTreeReader.h\"\r\n",
      "         ^\r\n",
      "input_line_36:1:10: fatal error: 'external/ExRootAnalysis/ExRootResult.h' file not found\r\n",
      "#include \"external/ExRootAnalysis/ExRootResult.h\"\r\n",
      "         ^\r\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd.variable import *\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import util\n",
    "from __future__ import print_function\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "save_path = '/bigdata/shared/HepSIM/np/'\n",
    "training = np.load(save_path + 'training1.npy')\n",
    "target = np.load(save_path + 'target1.npy')\n",
    "params = ['Px','Py', 'Pz', 'PT', 'E', 'D0', 'DZ', 'X', 'Y',  'Z', 'T', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file\n",
      "Generate dictionary\n",
      "['H', 'W+', 'Z', 'b', 'b', 'c', 'c'] 2\n",
      "['H', 'W+', 'Z', 'b', 'b', 'c'] 2\n",
      "['H', 'W+', 'Z', 'b', 'b', 's', 's'] 2\n",
      "['H', 'W+', 'Z', 'b', 'b', 's'] 2\n",
      "['H', 'W+', 'Z', 'b', 'b', 't'] 2\n",
      "['H', 'W+', 'Z', 'b', 'b'] 2\n",
      "['H', 'W+', 'Z', 'b'] 2\n",
      "['H', 'W+', 'Z', 'c', 'c'] 1\n",
      "['H', 'W+', 'Z', 'c'] 1\n",
      "['H', 'W+', 'Z', 's', 's'] 1\n",
      "['H', 'W+', 'Z', 's'] 1\n",
      "['H', 'W+', 'Z', 't'] 1\n",
      "['H', 'W+', 'Z'] 1\n",
      "['H', 'W+', 'b', 'b', 'c', 'c', 't'] 2\n",
      "['H', 'W+', 'b', 'b', 'c', 'c'] 2\n",
      "['H', 'W+', 'b', 'b', 'c', 's'] 2\n",
      "['H', 'W+', 'b', 'b', 'c', 't'] 2\n",
      "['H', 'W+', 'b', 'b', 'c'] 2\n",
      "['H', 'W+', 'b', 'b', 's', 's', 't'] 2\n",
      "['H', 'W+', 'b', 'b', 's', 's'] 2\n",
      "['H', 'W+', 'b', 'b', 's', 't'] 2\n",
      "['H', 'W+', 'b', 'b', 's'] 2\n",
      "['H', 'W+', 'b', 'b', 't'] 2\n",
      "['H', 'W+', 'b', 'b'] 2\n",
      "['H', 'W+', 'b', 't'] 2\n",
      "['H', 'W+', 'b'] 2\n",
      "['H', 'W+', 'c', 'c', 's', 's'] 1\n",
      "['H', 'W+', 'c', 'c', 's'] 1\n",
      "['H', 'W+', 'c', 'c'] 1\n",
      "['H', 'W+', 'c', 's', 's'] 1\n",
      "['H', 'W+', 'c', 's'] 1\n",
      "['H', 'W+', 'c'] 1\n",
      "['H', 'W+', 's', 's'] 1\n",
      "['H', 'W+', 's'] 1\n",
      "['H', 'W+', 't'] 1\n",
      "['H', 'W+'] 1\n",
      "['H', 'Z', 'b', 'b', 'c', 'c'] 2\n",
      "['H', 'Z', 'b', 'b', 'c', 's', 's'] 2\n",
      "['H', 'Z', 'b', 'b', 'c', 's'] 2\n",
      "['H', 'Z', 'b', 'b', 'c'] 2\n",
      "['H', 'Z', 'b', 'b', 's', 's'] 2\n",
      "['H', 'Z', 'b', 'b', 's'] 2\n",
      "['H', 'Z', 'b', 'b', 't'] 2\n",
      "['H', 'Z', 'b', 'b'] 2\n",
      "['H', 'Z', 'b', 'c', 'c'] 2\n",
      "['H', 'Z', 'b', 'c', 's'] 2\n",
      "['H', 'Z', 'b', 'c'] 2\n",
      "['H', 'Z', 'b', 's', 's'] 2\n",
      "['H', 'Z', 'b', 's'] 2\n",
      "['H', 'Z', 'b', 't'] 2\n",
      "['H', 'Z', 'b'] 2\n",
      "['H', 'Z', 'c', 'c'] 1\n",
      "['H', 'Z', 'c', 's'] 1\n",
      "['H', 'Z', 'c'] 1\n",
      "['H', 'Z', 's', 's'] 1\n",
      "['H', 'Z', 's'] 1\n",
      "['H', 'Z'] 1\n",
      "['H', 'b', 'b', 'c', 'c', 's', 's'] 2\n",
      "['H', 'b', 'b', 'c', 'c', 's'] 2\n",
      "['H', 'b', 'b', 'c', 'c'] 2\n",
      "['H', 'b', 'b', 'c', 's', 's'] 2\n",
      "['H', 'b', 'b', 'c', 's'] 2\n",
      "['H', 'b', 'b', 'c'] 2\n",
      "['H', 'b', 'b', 's', 's'] 2\n",
      "['H', 'b', 'b', 's'] 2\n",
      "['H', 'b', 'b', 't'] 2\n",
      "['H', 'b', 'b'] 2\n",
      "['H', 'b', 'c', 'c'] 2\n",
      "['H', 'b', 'c', 's'] 2\n",
      "['H', 'b', 'c'] 2\n",
      "['H', 'b', 's', 's'] 2\n",
      "['H', 'b', 's'] 2\n",
      "['H', 'b', 't'] 2\n",
      "['H', 'b'] 2\n",
      "['H', 'c', 'c', 's', 's'] 1\n",
      "['H', 'c', 'c', 's'] 1\n",
      "['H', 'c', 'c'] 1\n",
      "['H', 'c', 's', 's'] 1\n",
      "['H', 'c', 's'] 1\n",
      "['H', 'c'] 1\n",
      "['H', 's', 's'] 1\n",
      "['H', 's'] 1\n",
      "['H', 't'] 1\n",
      "['H'] 1\n",
      "['W+', 'Z', 'b', 'b', 'c', 'c', 's', 's'] 0\n",
      "['W+', 'Z', 'b', 'b', 'c', 'c', 's'] 0\n",
      "['W+', 'Z', 'b', 'b', 'c', 'c'] 0\n",
      "['W+', 'Z', 'b', 'b', 'c', 's', 's'] 0\n",
      "['W+', 'Z', 'b', 'b', 'c', 's'] 0\n",
      "['W+', 'Z', 'b', 'b', 'c'] 0\n",
      "['W+', 'Z', 'b', 'b', 's', 's', 't'] 0\n",
      "['W+', 'Z', 'b', 'b', 's', 's'] 0\n",
      "['W+', 'Z', 'b', 'b', 's'] 0\n",
      "['W+', 'Z', 'b', 'b', 't'] 0\n",
      "['W+', 'Z', 'b', 'b'] 0\n",
      "['W+', 'Z', 'b', 'c', 'c', 's', 's'] 0\n",
      "['W+', 'Z', 'b', 'c', 'c', 's', 't'] 0\n",
      "['W+', 'Z', 'b', 'c', 'c', 's'] 0\n",
      "['W+', 'Z', 'b', 'c', 'c', 't'] 0\n",
      "['W+', 'Z', 'b', 'c', 'c'] 0\n",
      "['W+', 'Z', 'b', 'c', 's', 's'] 0\n",
      "['W+', 'Z', 'b', 'c', 's'] 0\n",
      "['W+', 'Z', 'b', 'c'] 0\n",
      "['W+', 'Z', 'b', 's', 's'] 0\n",
      "['W+', 'Z', 'b', 's', 't'] 0\n",
      "['W+', 'Z', 'b', 's'] 0\n",
      "['W+', 'Z', 'b'] 0\n",
      "['W+', 'Z', 'c', 'c', 's', 's'] 0\n",
      "['W+', 'Z', 'c', 'c', 's'] 0\n",
      "['W+', 'Z', 'c', 'c'] 0\n",
      "['W+', 'Z', 'c', 's', 's'] 0\n",
      "['W+', 'Z', 's', 's'] 0\n",
      "['W+', 'Z', 's'] 0\n",
      "['W+', 'Z'] 0\n",
      "['W+', 'b', 'b', 'c', 'c', 't'] 0\n",
      "['W+', 'b', 'b', 'c', 't'] 0\n",
      "['W+', 'b', 'b', 's', 's', 't'] 0\n",
      "['W+', 'b', 'b', 's', 't'] 0\n",
      "['W+', 'b', 'b', 's'] 0\n",
      "['W+', 'b', 'b', 't'] 0\n",
      "['W+', 'b', 'b'] 0\n",
      "['W+', 'b', 'c', 's', 's'] 0\n",
      "['W+', 'b', 's', 's', 't'] 0\n",
      "['W+', 'b', 't'] 0\n",
      "['W+', 'b'] 0\n",
      "['W+', 's', 's'] 0\n",
      "['W+', 's'] 0\n",
      "['W+', 't'] 0\n",
      "['W+'] 0\n",
      "['Z', 'b', 'b', 'c', 'c', 's', 's'] 0\n",
      "['Z', 'b', 'b', 'c', 'c', 's'] 0\n",
      "['Z', 'b', 'b', 'c', 'c'] 0\n",
      "['Z', 'b', 'b', 'c', 's', 's'] 0\n",
      "['Z', 'b', 'b', 'c'] 0\n",
      "['Z', 'b', 'b', 's', 's'] 0\n",
      "['Z', 'b', 'b', 's'] 0\n",
      "['Z', 'b', 'b', 't'] 0\n",
      "['Z', 'b', 'b'] 0\n",
      "['Z', 'b', 'c', 's', 's'] 0\n",
      "['Z', 'b', 'c'] 0\n",
      "['Z', 'b', 's', 's'] 0\n",
      "['Z', 'b', 't'] 0\n",
      "['Z', 'b'] 0\n",
      "['Z', 's', 's'] 0\n",
      "['Z'] 0\n",
      "['b', 'b', 'c'] 0\n",
      "['b', 'b', 's', 's'] 0\n",
      "['b', 'b', 's', 't'] 0\n",
      "['b', 'b', 's'] 0\n",
      "['b', 'b', 't'] 0\n",
      "['b', 'b'] 0\n",
      "['b', 't'] 0\n",
      "['b'] 0\n",
      "['t'] 0\n",
      "Assigning jet_type...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "util.py:359: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  l = np.zeros(length)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1490621, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import util\n",
    "N = 100\n",
    "df = util.h5_to_df(\"/bigdata/shared/HepSIM/combo/pythia8_higgs_2_combo.h5\")\n",
    "params = ['Px','Py', 'Pz', 'PT', 'E', 'D0', 'DZ', 'X', 'Y',  'Z', 'T', 'count']\n",
    "training, target = util.df_to_target(df, output = None, params = params, max_len = N)\n",
    "training = np.einsum('ijk->ikj', training)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Loss: 0.89694 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3009/5053 = 59.5487829012%\n",
      "  Target 1: 2574/4969 = 51.8011672369%\n",
      "  Target 2: 3011/4978 = 60.4861390117%\n",
      "Overall: 8594/15000 = 57.2933333333%\n",
      "Epoch 1\n",
      "Loss: 0.80015 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3137/5053 = 62.0819315258%\n",
      "  Target 1: 3007/4969 = 60.5151942041%\n",
      "  Target 2: 3090/4978 = 62.0731217356%\n",
      "Overall: 9234/15000 = 61.56%\n",
      "Epoch 2\n",
      "Loss: 0.73639 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3447/5053 = 68.216900851%\n",
      "  Target 1: 3327/4969 = 66.9551217549%\n",
      "  Target 2: 2918/4978 = 58.6179188429%\n",
      "Overall: 9692/15000 = 64.6133333333%\n",
      "Epoch 3\n",
      "Loss: 0.70642 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3611/5053 = 71.4624975262%\n",
      "  Target 1: 3072/4969 = 61.8233044878%\n",
      "  Target 2: 3204/4978 = 64.3631980715%\n",
      "Overall: 9887/15000 = 65.9133333333%\n",
      "Epoch 4\n",
      "Loss: 0.69932 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3744/5053 = 74.0945972689%\n",
      "  Target 1: 3057/4969 = 61.5214328839%\n",
      "  Target 2: 3170/4978 = 63.6801928485%\n",
      "Overall: 9971/15000 = 66.4733333333%\n",
      "Epoch 5\n",
      "Loss: 0.69062 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3809/5053 = 75.3809618049%\n",
      "  Target 1: 3511/4969 = 70.6580800966%\n",
      "  Target 2: 2824/4978 = 56.7296102853%\n",
      "Overall: 10144/15000 = 67.6266666667%\n",
      "Epoch 6\n",
      "Loss: 0.68559 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3851/5053 = 76.2121511973%\n",
      "  Target 1: 3372/4969 = 67.8607365667%\n",
      "  Target 2: 2962/4978 = 59.501807955%\n",
      "Overall: 10185/15000 = 67.9%\n",
      "Epoch 7\n",
      "Loss: 0.68427 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3881/5053 = 76.8058579062%\n",
      "  Target 1: 3499/4969 = 70.4165828134%\n",
      "  Target 2: 2845/4978 = 57.1514664524%\n",
      "Overall: 10225/15000 = 68.1666666667%\n",
      "Epoch 8\n",
      "Loss: 0.68157 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3900/5053 = 77.1818721552%\n",
      "  Target 1: 3567/4969 = 71.785067418%\n",
      "  Target 2: 2802/4978 = 56.2876657292%\n",
      "Overall: 10269/15000 = 68.46%\n",
      "Epoch 9\n",
      "Loss: 0.67838 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3935/5053 = 77.8745299822%\n",
      "  Target 1: 3661/4969 = 73.676796136%\n",
      "  Target 2: 2696/4978 = 54.1582965046%\n",
      "Overall: 10292/15000 = 68.6133333333%\n",
      "Epoch 10\n",
      "Loss: 0.67736 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3940/5053 = 77.9734811003%\n",
      "  Target 1: 3591/4969 = 72.2680619843%\n",
      "  Target 2: 2789/4978 = 56.0265166734%\n",
      "Overall: 10320/15000 = 68.8%\n",
      "Epoch 11\n",
      "Loss: 0.67443 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3960/5053 = 78.3692855729%\n",
      "  Target 1: 3672/4969 = 73.8981686456%\n",
      "  Target 2: 2724/4978 = 54.7207713941%\n",
      "Overall: 10356/15000 = 69.04%\n",
      "Epoch 12\n",
      "Loss: 0.67232 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3933/5053 = 77.8349495349%\n",
      "  Target 1: 3602/4969 = 72.4894344939%\n",
      "  Target 2: 2810/4978 = 56.4483728405%\n",
      "Overall: 10345/15000 = 68.9666666667%\n",
      "Epoch 13\n",
      "Loss: 0.67144 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3916/5053 = 77.4985157332%\n",
      "  Target 1: 3508/4969 = 70.5977057758%\n",
      "  Target 2: 2922/4978 = 58.6982723986%\n",
      "Overall: 10346/15000 = 68.9733333333%\n",
      "Epoch 14\n",
      "Loss: 0.67087 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3910/5053 = 77.3797743915%\n",
      "  Target 1: 3563/4969 = 71.7045683236%\n",
      "  Target 2: 2888/4978 = 58.0152671756%\n",
      "Overall: 10361/15000 = 69.0733333333%\n",
      "Epoch 15\n",
      "Loss: 0.66962 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3911/5053 = 77.3995646151%\n",
      "  Target 1: 3443/4969 = 69.2895954921%\n",
      "  Target 2: 3004/4978 = 60.3455202893%\n",
      "Overall: 10358/15000 = 69.0533333333%\n",
      "Epoch 16\n",
      "Loss: 0.66695 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3906/5053 = 77.3006134969%\n",
      "  Target 1: 3017/4969 = 60.71644194%\n",
      "  Target 2: 3283/4978 = 65.9501807955%\n",
      "Overall: 10206/15000 = 68.04%\n",
      "Epoch 17\n",
      "Loss: 0.66658 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3910/5053 = 77.3797743915%\n",
      "  Target 1: 3334/4969 = 67.0959951701%\n",
      "  Target 2: 3100/4978 = 62.2740056247%\n",
      "Overall: 10344/15000 = 68.96%\n",
      "Epoch 18\n",
      "Loss: 0.66540 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3907/5053 = 77.3204037206%\n",
      "  Target 1: 3439/4969 = 69.2090963977%\n",
      "  Target 2: 3033/4978 = 60.9280835677%\n",
      "Overall: 10379/15000 = 69.1933333333%\n",
      "Epoch 19\n",
      "Loss: 0.66394 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3907/5053 = 77.3204037206%\n",
      "  Target 1: 3291/4969 = 66.2306299054%\n",
      "  Target 2: 3133/4978 = 62.9369224588%\n",
      "Overall: 10331/15000 = 68.8733333333%\n",
      "Epoch 20\n",
      "Loss: 0.66208 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3905/5053 = 77.2808232733%\n",
      "  Target 1: 3366/4969 = 67.7399879251%\n",
      "  Target 2: 3083/4978 = 61.9325030133%\n",
      "Overall: 10354/15000 = 69.0266666667%\n",
      "Epoch 21\n",
      "Loss: 0.66210 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3937/5053 = 77.9141104294%\n",
      "  Target 1: 4107/4969 = 82.65244516%\n",
      "  Target 2: 2453/4978 = 49.2768179992%\n",
      "Overall: 10497/15000 = 69.98%\n",
      "Epoch 22\n",
      "Loss: 0.65901 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3931/5053 = 77.7953690877%\n",
      "  Target 1: 3674/4969 = 73.9384181928%\n",
      "  Target 2: 2829/4978 = 56.8300522298%\n",
      "Overall: 10434/15000 = 69.56%\n",
      "Epoch 23\n",
      "Loss: 0.65805 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3931/5053 = 77.7953690877%\n",
      "  Target 1: 3392/4969 = 68.2632320386%\n",
      "  Target 2: 3041/4978 = 61.088790679%\n",
      "Overall: 10364/15000 = 69.0933333333%\n",
      "Epoch 24\n",
      "Loss: 0.65703 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3929/5053 = 77.7557886404%\n",
      "  Target 1: 3592/4969 = 72.2881867579%\n",
      "  Target 2: 2928/4978 = 58.818802732%\n",
      "Overall: 10449/15000 = 69.66%\n",
      "Epoch 25\n",
      "Loss: 0.65604 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3935/5053 = 77.8745299822%\n",
      "  Target 1: 3645/4969 = 73.3547997585%\n",
      "  Target 2: 2887/4978 = 57.9951787867%\n",
      "Overall: 10467/15000 = 69.78%\n",
      "Epoch 26\n",
      "Loss: 0.65516 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3937/5053 = 77.9141104294%\n",
      "  Target 1: 3507/4969 = 70.5775810022%\n",
      "  Target 2: 2953/4978 = 59.3210124548%\n",
      "Overall: 10397/15000 = 69.3133333333%\n",
      "Epoch 27\n",
      "Loss: 0.65290 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3929/5053 = 77.7557886404%\n",
      "  Target 1: 3497/4969 = 70.3763332663%\n",
      "  Target 2: 2989/4978 = 60.0441944556%\n",
      "Overall: 10415/15000 = 69.4333333333%\n",
      "Epoch 28\n",
      "Loss: 0.65343 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3929/5053 = 77.7557886404%\n",
      "  Target 1: 3508/4969 = 70.5977057758%\n",
      "  Target 2: 2985/4978 = 59.9638409%\n",
      "Overall: 10422/15000 = 69.48%\n",
      "Epoch 29\n",
      "Loss: 0.65118 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3947/5053 = 78.1120126657%\n",
      "  Target 1: 3779/4969 = 76.0515194204%\n",
      "  Target 2: 2795/4978 = 56.1470470068%\n",
      "Overall: 10521/15000 = 70.14%\n",
      "Epoch 30\n",
      "Loss: 0.64868 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3942/5053 = 78.0130615476%\n",
      "  Target 1: 3889/4969 = 78.265244516%\n",
      "  Target 2: 2708/4978 = 54.3993571716%\n",
      "Overall: 10539/15000 = 70.26%\n",
      "Epoch 31\n",
      "Loss: 0.64871 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3967/5053 = 78.5078171383%\n",
      "  Target 1: 3797/4969 = 76.4137653451%\n",
      "  Target 2: 2776/4978 = 55.7653676175%\n",
      "Overall: 10540/15000 = 70.2666666667%\n",
      "Epoch 32\n",
      "Loss: 0.64772 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3971/5053 = 78.5869780329%\n",
      "  Target 1: 3871/4969 = 77.9029985913%\n",
      "  Target 2: 2712/4978 = 54.4797107272%\n",
      "Overall: 10554/15000 = 70.36%\n",
      "Epoch 33\n",
      "Loss: 0.64692 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3950/5053 = 78.1713833366%\n",
      "  Target 1: 3875/4969 = 77.9834976857%\n",
      "  Target 2: 2713/4978 = 54.4997991161%\n",
      "Overall: 10538/15000 = 70.2533333333%\n",
      "Epoch 34\n",
      "Loss: 0.64440 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3978/5053 = 78.7255095983%\n",
      "  Target 1: 3951/4969 = 79.512980479%\n",
      "  Target 2: 2641/4978 = 53.0534351145%\n",
      "Overall: 10570/15000 = 70.4666666667%\n",
      "Epoch 35\n",
      "Loss: 0.64478 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3962/5053 = 78.4088660202%\n",
      "  Target 1: 4019/4969 = 80.8814650835%\n",
      "  Target 2: 2583/4978 = 51.8883085577%\n",
      "Overall: 10564/15000 = 70.4266666667%\n",
      "Epoch 36\n",
      "Loss: 0.64443 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3977/5053 = 78.7057193746%\n",
      "  Target 1: 4094/4969 = 82.3908231032%\n",
      "  Target 2: 2500/4978 = 50.220972278%\n",
      "Overall: 10571/15000 = 70.4733333333%\n",
      "Epoch 37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.64556 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3971/5053 = 78.5869780329%\n",
      "  Target 1: 4015/4969 = 80.8009659891%\n",
      "  Target 2: 2577/4978 = 51.7677782242%\n",
      "Overall: 10563/15000 = 70.42%\n",
      "Epoch 38\n",
      "Loss: 0.64328 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3962/5053 = 78.4088660202%\n",
      "  Target 1: 4031/4969 = 81.1229623667%\n",
      "  Target 2: 2571/4978 = 51.6472478907%\n",
      "Overall: 10564/15000 = 70.4266666667%\n",
      "Epoch 39\n",
      "Loss: 0.64018 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3968/5053 = 78.527607362%\n",
      "  Target 1: 3873/4969 = 77.9432481385%\n",
      "  Target 2: 2674/4978 = 53.7163519486%\n",
      "Overall: 10515/15000 = 70.1%\n",
      "Epoch 40\n",
      "Loss: 0.64270 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3952/5053 = 78.2109637839%\n",
      "  Target 1: 3723/4969 = 74.924532099%\n",
      "  Target 2: 2813/4978 = 56.5086380072%\n",
      "Overall: 10488/15000 = 69.92%\n",
      "Epoch 41\n",
      "Loss: 0.64048 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3939/5053 = 77.9536908767%\n",
      "  Target 1: 3952/4969 = 79.5331052526%\n",
      "  Target 2: 2669/4978 = 53.615910004%\n",
      "Overall: 10560/15000 = 70.4%\n",
      "Epoch 42\n",
      "Loss: 0.64074 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3957/5053 = 78.309914902%\n",
      "  Target 1: 3918/4969 = 78.8488629503%\n",
      "  Target 2: 2690/4978 = 54.0377661712%\n",
      "Overall: 10565/15000 = 70.4333333333%\n",
      "Epoch 43\n",
      "Loss: 0.64003 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3959/5053 = 78.3494953493%\n",
      "  Target 1: 3906/4969 = 78.6073656671%\n",
      "  Target 2: 2705/4978 = 54.3390920048%\n",
      "Overall: 10570/15000 = 70.4666666667%\n",
      "Epoch 44\n",
      "Loss: 0.63898 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3965/5053 = 78.4682366911%\n",
      "  Target 1: 4042/4969 = 81.3443348762%\n",
      "  Target 2: 2593/4978 = 52.0891924468%\n",
      "Overall: 10600/15000 = 70.6666666667%\n",
      "Epoch 45\n",
      "Loss: 0.64070 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3961/5053 = 78.3890757966%\n",
      "  Target 1: 4148/4969 = 83.4775608774%\n",
      "  Target 2: 2528/4978 = 50.7834471675%\n",
      "Overall: 10637/15000 = 70.9133333333%\n",
      "Epoch 46\n",
      "Loss: 0.63733 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3961/5053 = 78.3890757966%\n",
      "  Target 1: 4061/4969 = 81.7267055746%\n",
      "  Target 2: 2587/4978 = 51.9686621133%\n",
      "Overall: 10609/15000 = 70.7266666667%\n",
      "Epoch 47\n",
      "Loss: 0.63820 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3937/5053 = 77.9141104294%\n",
      "  Target 1: 4066/4969 = 81.8273294425%\n",
      "  Target 2: 2598/4978 = 52.1896343913%\n",
      "Overall: 10601/15000 = 70.6733333333%\n",
      "Epoch 48\n",
      "Loss: 0.63690 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3948/5053 = 78.1318028894%\n",
      "  Target 1: 3973/4969 = 79.9557254981%\n",
      "  Target 2: 2687/4978 = 53.9775010044%\n",
      "Overall: 10608/15000 = 70.72%\n",
      "Epoch 49\n",
      "Loss: 0.63605 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3954/5053 = 78.2505442311%\n",
      "  Target 1: 3935/4969 = 79.1909841014%\n",
      "  Target 2: 2727/4978 = 54.7810365609%\n",
      "Overall: 10616/15000 = 70.7733333333%\n",
      "Epoch 50\n",
      "Loss: 0.63551 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3921/5053 = 77.5974668514%\n",
      "  Target 1: 3953/4969 = 79.5532300262%\n",
      "  Target 2: 2720/4978 = 54.6404178385%\n",
      "Overall: 10594/15000 = 70.6266666667%\n",
      "Epoch 51\n",
      "Loss: 0.63559 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3950/5053 = 78.1713833366%\n",
      "  Target 1: 4031/4969 = 81.1229623667%\n",
      "  Target 2: 2626/4978 = 52.7521092808%\n",
      "Overall: 10607/15000 = 70.7133333333%\n",
      "Epoch 52\n",
      "Loss: 0.63489 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3952/5053 = 78.2109637839%\n",
      "  Target 1: 4048/4969 = 81.4650835178%\n",
      "  Target 2: 2622/4978 = 52.6717557252%\n",
      "Overall: 10622/15000 = 70.8133333333%\n",
      "Epoch 53\n",
      "Loss: 0.63500 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3942/5053 = 78.0130615476%\n",
      "  Target 1: 4063/4969 = 81.7669551218%\n",
      "  Target 2: 2611/4978 = 52.4507834472%\n",
      "Overall: 10616/15000 = 70.7733333333%\n",
      "Epoch 54\n",
      "Loss: 0.63492 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3934/5053 = 77.8547397586%\n",
      "  Target 1: 4064/4969 = 81.7870798954%\n",
      "  Target 2: 2610/4978 = 52.4306950583%\n",
      "Overall: 10608/15000 = 70.72%\n",
      "Epoch 55\n",
      "Loss: 0.63439 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3956/5053 = 78.2901246784%\n",
      "  Target 1: 3967/4969 = 79.8349768565%\n",
      "  Target 2: 2692/4978 = 54.077942949%\n",
      "Overall: 10615/15000 = 70.7666666667%\n",
      "Epoch 56\n",
      "Loss: 0.63639 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3957/5053 = 78.309914902%\n",
      "  Target 1: 4084/4969 = 82.1895753673%\n",
      "  Target 2: 2584/4978 = 51.9083969466%\n",
      "Overall: 10625/15000 = 70.8333333333%\n",
      "Epoch 57\n",
      "Loss: 0.63507 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3971/5053 = 78.5869780329%\n",
      "  Target 1: 4094/4969 = 82.3908231032%\n",
      "  Target 2: 2552/4978 = 51.2655685014%\n",
      "Overall: 10617/15000 = 70.78%\n",
      "Epoch 58\n",
      "Loss: 0.63748 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3959/5053 = 78.3494953493%\n",
      "  Target 1: 4048/4969 = 81.4650835178%\n",
      "  Target 2: 2569/4978 = 51.6070711129%\n",
      "Overall: 10576/15000 = 70.5066666667%\n",
      "Epoch 59\n",
      "Loss: 0.63526 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3961/5053 = 78.3890757966%\n",
      "  Target 1: 4190/4969 = 84.3228013685%\n",
      "  Target 2: 2485/4978 = 49.9196464444%\n",
      "Overall: 10636/15000 = 70.9066666667%\n",
      "Epoch 60\n",
      "Loss: 0.63411 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3951/5053 = 78.1911735603%\n",
      "  Target 1: 4034/4969 = 81.1833366875%\n",
      "  Target 2: 2626/4978 = 52.7521092808%\n",
      "Overall: 10611/15000 = 70.74%\n",
      "Epoch 61\n",
      "Loss: 0.63818 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3952/5053 = 78.2109637839%\n",
      "  Target 1: 4078/4969 = 82.0688267257%\n",
      "  Target 2: 2580/4978 = 51.8280433909%\n",
      "Overall: 10610/15000 = 70.7333333333%\n",
      "Epoch 62\n",
      "Loss: 0.63482 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3948/5053 = 78.1318028894%\n",
      "  Target 1: 4127/4969 = 83.0549406319%\n",
      "  Target 2: 2556/4978 = 51.3459220571%\n",
      "Overall: 10631/15000 = 70.8733333333%\n",
      "Epoch 63\n",
      "Loss: 0.63787 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3959/5053 = 78.3494953493%\n",
      "  Target 1: 4111/4969 = 82.7329442544%\n",
      "  Target 2: 2563/4978 = 51.4865407794%\n",
      "Overall: 10633/15000 = 70.8866666667%\n",
      "Epoch 64\n",
      "Loss: 0.63494 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3947/5053 = 78.1120126657%\n",
      "  Target 1: 3971/4969 = 79.9154759509%\n",
      "  Target 2: 2675/4978 = 53.7364403375%\n",
      "Overall: 10593/15000 = 70.62%\n",
      "Epoch 65\n",
      "Loss: 0.63771 [135000/135000]  |####################| 100.0% \n",
      "  Target 0: 3921/5053 = 77.5974668514%\n",
      "  Target 1: 4197/4969 = 84.4636747837%\n",
      "  Target 2: 2514/4978 = 50.5022097228%\n",
      "Overall: 10632/15000 = 70.88%\n",
      "Epoch 66\n",
      "Loss: 0.60331 [77000/135000]  |###########---------| 57.0% "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e771c1e16df9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mloss_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Loss: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m\"{0:.5f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         util.printProgressBar(j + batch_size, trainingv.size()[0], \n\u001b[1;32m    107\u001b[0m                               prefix = \"%s [%s/%s] \" % (loss_string, \n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/_functions/tensor.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdest_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdest_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/_utils.pyc\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "class GraphNet(nn.Module):\n",
    "    def __init__(self, n_constituents, n_targets, params, hidden):\n",
    "        super(GraphNet, self).__init__()\n",
    "        self.hidden = hidden\n",
    "        self.P = len(params)\n",
    "        self.N = n_constituents\n",
    "        self.Nr = self.N * (self.N - 1)\n",
    "        self.Dr = 0\n",
    "        self.De = 5\n",
    "        self.Dx = 0\n",
    "        self.Do = 6\n",
    "        self.n_targets = n_targets\n",
    "        self.assign_matrices()\n",
    "\n",
    "        self.Ra = Variable(torch.ones(self.Dr, self.Nr))\n",
    "        self.fr1 = nn.Linear(2 * self.P + self.Dr, hidden).cuda()\n",
    "        self.fr2 = nn.Linear(hidden, hidden/2).cuda()\n",
    "        self.fr3 = nn.Linear(hidden/2, self.De).cuda()\n",
    "        self.fo1 = nn.Linear(self.P + self.Dx + self.De, hidden).cuda()\n",
    "        self.fo2 = nn.Linear(hidden, hidden/2).cuda()\n",
    "        self.fo3 = nn.Linear(hidden/2, self.Do).cuda()\n",
    "        self.fc1 = nn.Linear(self.Do * self.N, hidden).cuda()\n",
    "        self.fc2 = nn.Linear(hidden, hidden/2).cuda()\n",
    "        self.fc3 = nn.Linear(hidden/2, self.n_targets).cuda()\n",
    "    \n",
    "    def assign_matrices(self):\n",
    "        self.Rr = torch.zeros(self.N, self.Nr)\n",
    "        self.Rs = torch.zeros(self.N, self.Nr)\n",
    "        receiver_sender_list = [i for i in itertools.product(range(self.N), range(self.N)) if i[0]!=i[1]]\n",
    "        for i, (r, s) in enumerate(receiver_sender_list):\n",
    "            self.Rr[r, i] = 1\n",
    "            self.Rs[s, i] = 1\n",
    "        self.Rr = Variable(self.Rr).cuda()\n",
    "        self.Rs = Variable(self.Rs).cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Orr = self.tmul(x, self.Rr)\n",
    "        Ors = self.tmul(x, self.Rs)\n",
    "        B = torch.cat([Orr, Ors], 1)\n",
    "        ### First MLP ###\n",
    "        B = torch.transpose(B, 1, 2).contiguous()\n",
    "        B = nn.functional.relu(self.fr1(B.view(-1, 2 * self.P + self.Dr)))\n",
    "        B = nn.functional.relu(self.fr2(B))\n",
    "        E = nn.functional.relu(self.fr3(B).view(-1, self.Nr, self.De))\n",
    "        del B\n",
    "        E = torch.transpose(E, 1, 2).contiguous()\n",
    "        Ebar = self.tmul(E, torch.transpose(self.Rr, 0, 1).contiguous())\n",
    "        del E\n",
    "        C = torch.cat([x, Ebar], 1)\n",
    "        C = torch.transpose(C, 1, 2).contiguous()\n",
    "        ### Second MLP ###\n",
    "        C = nn.functional.relu(self.fo1(C.view(-1, self.P + self.Dx + self.De)))\n",
    "        C = nn.functional.relu(self.fo2(C))\n",
    "        O = nn.functional.relu(self.fo3(C).view(-1, self.N, self.Do))\n",
    "        del C\n",
    "        ### Classification MLP ###\n",
    "        N = nn.functional.relu(self.fc1(O.view(-1, self.Do * self.N)))\n",
    "        del O\n",
    "        N = nn.functional.relu(self.fc2(N))\n",
    "        N = nn.functional.relu(self.fc3(N))\n",
    "        return N\n",
    "\n",
    "    def tmul(self, x, y):  #Takes (I * J * K)(K * L) -> I * J * L \n",
    "        x_shape = x.size()\n",
    "        y_shape = y.size()\n",
    "        return torch.mm(x.view(-1, x_shape[2]), y).view(-1, x_shape[1], y_shape[1])\n",
    "    \n",
    "def get_sample(training, target, choice):\n",
    "    target_vals = np.argmax(target, axis = 1)\n",
    "    ind, = np.where(target_vals == choice)\n",
    "    chosen_ind = np.random.choice(ind, 50000)\n",
    "    return training[chosen_ind], target[chosen_ind]\n",
    "\n",
    "n_targets = target.shape[1]\n",
    "samples = [get_sample(training, target, i) for i in range(n_targets)]\n",
    "trainings = [i[0] for i in samples]\n",
    "targets = [i[1] for i in samples]\n",
    "big_training = np.concatenate(trainings)\n",
    "big_target = np.concatenate(targets)\n",
    "big_training, big_target = util.shuffle_together(big_training, big_target)\n",
    "\n",
    "val_split = 0.1\n",
    "batch_size = 1000\n",
    "n_epochs = 100\n",
    "\n",
    "trainingv = Variable(torch.FloatTensor(big_training))\n",
    "targetv = Variable(torch.from_numpy(np.argmax(big_target, axis = 1)).long())  \n",
    "trainingv, valv = torch.split(trainingv, int(trainingv.size()[0] * (1 - val_split)))\n",
    "targetv, val_targetv = torch.split(targetv, int(targetv.size()[0] * (1 - val_split)))\n",
    "gnn = GraphNet(N, n_targets, params, 10)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gnn.parameters())\n",
    "loss_vals = np.zeros(n_epochs)\n",
    "acc_vals = np.zeros(n_epochs)\n",
    "for i in range(n_epochs):\n",
    "    print(\"Epoch %s\" % i)\n",
    "    for j in range(0, trainingv.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        out = gnn(trainingv[j:j + batch_size].cuda())\n",
    "        l = loss(out, targetv[j:j + batch_size].cuda())\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        loss_string = \"Loss: %s\" % \"{0:.5f}\".format(l.cpu().data.numpy()[0])\n",
    "        util.printProgressBar(j + batch_size, trainingv.size()[0], \n",
    "                              prefix = \"%s [%s/%s] \" % (loss_string, \n",
    "                                                       j + batch_size, \n",
    "                                                       trainingv.size()[0]),\n",
    "                              length = 20)\n",
    "    lst = []\n",
    "    for j in torch.split(valv, 100):\n",
    "        a = gnn(j.cuda()).cpu().data.numpy()\n",
    "        lst.append(a)\n",
    "    predicted = Variable(torch.FloatTensor(np.concatenate(lst)))\n",
    "    acc_vals[i] = stats(predicted, val_targetv)\n",
    "    loss_vals[i] = l.cpu().data.numpy()[0]\n",
    "    if all(loss_vals[max(0, i - 5):i] > min(np.append(loss_vals[0:max(0, i - 5)], 200))) and i > 5:\n",
    "        print(loss_vals, '\\n', np.diff(loss_vals))\n",
    "        break\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gnn = GraphNet(3, 4, ['Px'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predict, target):\n",
    "    _, p_vals = torch.max(predict, 1)\n",
    "    r = torch.sum(target == p_vals.squeeze(1)).data.numpy()[0]\n",
    "    t = target.size()[0]\n",
    "    return r * 1.0 / t\n",
    "\n",
    "def stats(predict, target):\n",
    "    _, p_vals = torch.max(predict, 1)\n",
    "    t = target.cpu().data.numpy()\n",
    "    p_vals = p_vals.squeeze(1).data.numpy()\n",
    "    vals = np.unique(t)\n",
    "    for i in vals:\n",
    "        ind = np.where(t == i)\n",
    "        pv = p_vals[ind]\n",
    "        correct = sum(pv == t[ind])\n",
    "        print(\"  Target %s: %s/%s = %s%%\" % (i, correct, len(pv), correct * 100.0/len(pv)))\n",
    "    print(\"Overall: %s/%s = %s%%\" % (sum(p_vals == t), len(t), sum(p_vals == t) * 100.0/len(t)))\n",
    "    return sum(p_vals == t) * 100.0/len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "def get_cmap(n, name='hsv'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n + 1)\n",
    "\n",
    "def predicted_histogram(data, \n",
    "                        target, \n",
    "                        labels = None, \n",
    "                        nbins = 10, \n",
    "                        out = None,\n",
    "                        xlabel = None,\n",
    "                        title = None\n",
    "                       ):\n",
    "    \"\"\"@params:\n",
    "        data = n x 1 array of parameter values\n",
    "        target = n x categories array of predictions\n",
    "    \"\"\"\n",
    "    target = preprocessing.normalize(target, norm = \"l1\")\n",
    "    if labels == None:\n",
    "        labels = [\"\" for i in range(target.shape[1])]\n",
    "    #1 decide bins\n",
    "    ma = np.amax(data) * 1.0\n",
    "    mi = np.amin(data)\n",
    "    bins = np.linspace(mi, ma, nbins)\n",
    "    bin_size = bins[1] - bins[0]\n",
    "    bin_locs = np.digitize(data, bins, right = True)\n",
    "    #2 set up bin x category matrix\n",
    "    #  Each M(bin, category) = Sum over particles with param in bin of category\n",
    "    M = np.array([np.sum(target[np.where(bin_locs == i)], axis = 0) \n",
    "                  for i in range(nbins)])\n",
    "    #3 plot each category/bin\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "    bars = np.array([M[:, i] for i in range(M.shape[1])])\n",
    "    cmap = get_cmap(len(bars), 'viridis')\n",
    "    for i in range(len(bars)):\n",
    "        ax.bar(bins, bars[i], \n",
    "               bottom = sum(bars[:i]), \n",
    "               color = cmap(i), \n",
    "               label = labels[i],\n",
    "               width = bin_size\n",
    "              )\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "def generate_control_plots():\n",
    "    len_params = len(params)\n",
    "    path = '/bigdata/shared/HepSIM/img/n-h-hb/'\n",
    "    os.makedirs(path)\n",
    "    pred = gnn(valv)\n",
    "    d_target = np.array([util.get_list_from_num(i, length = n_targets) \n",
    "                             for i in val_targetv.cpu().data.numpy()])\n",
    "    p_target = pred.cpu().data.numpy()\n",
    "    for i in range(len(params)):\n",
    "        xlabel = params[i]\n",
    "        labels = [\"None\", \"H\", \"H + b\"]\n",
    "        data = np.mean(valv.data.numpy()[:, i, :], axis = 1)\n",
    "        predicted_histogram(data, d_target, \n",
    "                            nbins = 50, labels = labels,\n",
    "                            xlabel = xlabel, \n",
    "                            title = \"Actual Distribution\"\n",
    "                           )\n",
    "        plt.savefig(path + xlabel + \"-actual.png\", dpi = 200)\n",
    "        predicted_histogram(data, p_target, \n",
    "                            nbins = 50, labels = labels,\n",
    "                            xlabel = xlabel,\n",
    "                            title = \"Predicted Distribution\"\n",
    "                           )\n",
    "        plt.savefig(path + xlabel + \"-predicted.png\", dpi = 200)\n",
    "        plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "path = '/bigdata/shared/HepSIM/img/1ep-'\n",
    "display(Image(filename=path + 'Px-actual.png'))\n",
    "display(Image(filename=path + 'Px-predicted.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_control_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = '/bigdata/shared/HepSIM/np/'\n",
    "np.save(save_path + 'traininght.npy', training)\n",
    "np.save(save_path + 'targetht.npy', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.FloatTensor(np.concatenate(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.parents.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = util.h5_to_df(\"/bigdata/shared/HepSIM/combo/pythia8_higgs_2_combo.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(df.parents.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[df.parents == \"['Z', 'b', 't']\"].njet.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 57.29333333,  61.56      ,  64.61333333,  65.91333333,\n",
       "        66.47333333,  67.62666667,  67.9       ,  68.16666667,\n",
       "        68.46      ,  68.61333333,  68.8       ,  69.04      ,\n",
       "        68.96666667,  68.97333333,  69.07333333,  69.05333333,\n",
       "        68.04      ,  68.96      ,  69.19333333,  68.87333333,\n",
       "        69.02666667,  69.98      ,  69.56      ,  69.09333333,\n",
       "        69.66      ,  69.78      ,  69.31333333,  69.43333333,\n",
       "        69.48      ,  70.14      ,  70.26      ,  70.26666667,\n",
       "        70.36      ,  70.25333333,  70.46666667,  70.42666667,\n",
       "        70.47333333,  70.42      ,  70.42666667,  70.1       ,\n",
       "        69.92      ,  70.4       ,  70.43333333,  70.46666667,\n",
       "        70.66666667,  70.91333333,  70.72666667,  70.67333333,\n",
       "        70.72      ,  70.77333333,  70.62666667,  70.71333333,\n",
       "        70.81333333,  70.77333333,  70.72      ,  70.76666667,\n",
       "        70.83333333,  70.78      ,  70.50666667,  70.90666667,\n",
       "        70.74      ,  70.73333333,  70.87333333,  70.88666667,\n",
       "        70.62      ,  70.88      ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(np.append(np.array([]), 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 0\n",
    "Loss: 0.89694 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3009/5053 = 59.5487829012%\n",
    "  Target 1: 2574/4969 = 51.8011672369%\n",
    "  Target 2: 3011/4978 = 60.4861390117%\n",
    "Overall: 8594/15000 = 57.2933333333%\n",
    "Epoch 1\n",
    "Loss: 0.80015 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3137/5053 = 62.0819315258%\n",
    "  Target 1: 3007/4969 = 60.5151942041%\n",
    "  Target 2: 3090/4978 = 62.0731217356%\n",
    "Overall: 9234/15000 = 61.56%\n",
    "Epoch 2\n",
    "Loss: 0.73639 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3447/5053 = 68.216900851%\n",
    "  Target 1: 3327/4969 = 66.9551217549%\n",
    "  Target 2: 2918/4978 = 58.6179188429%\n",
    "Overall: 9692/15000 = 64.6133333333%\n",
    "Epoch 3\n",
    "Loss: 0.70642 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3611/5053 = 71.4624975262%\n",
    "  Target 1: 3072/4969 = 61.8233044878%\n",
    "  Target 2: 3204/4978 = 64.3631980715%\n",
    "Overall: 9887/15000 = 65.9133333333%\n",
    "Epoch 4\n",
    "Loss: 0.69932 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3744/5053 = 74.0945972689%\n",
    "  Target 1: 3057/4969 = 61.5214328839%\n",
    "  Target 2: 3170/4978 = 63.6801928485%\n",
    "Overall: 9971/15000 = 66.4733333333%\n",
    "Epoch 5\n",
    "Loss: 0.69062 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3809/5053 = 75.3809618049%\n",
    "  Target 1: 3511/4969 = 70.6580800966%\n",
    "  Target 2: 2824/4978 = 56.7296102853%\n",
    "Overall: 10144/15000 = 67.6266666667%\n",
    "Epoch 6\n",
    "Loss: 0.68559 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3851/5053 = 76.2121511973%\n",
    "  Target 1: 3372/4969 = 67.8607365667%\n",
    "  Target 2: 2962/4978 = 59.501807955%\n",
    "Overall: 10185/15000 = 67.9%\n",
    "Epoch 7\n",
    "Loss: 0.68427 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3881/5053 = 76.8058579062%\n",
    "  Target 1: 3499/4969 = 70.4165828134%\n",
    "  Target 2: 2845/4978 = 57.1514664524%\n",
    "Overall: 10225/15000 = 68.1666666667%\n",
    "Epoch 8\n",
    "Loss: 0.68157 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3900/5053 = 77.1818721552%\n",
    "  Target 1: 3567/4969 = 71.785067418%\n",
    "  Target 2: 2802/4978 = 56.2876657292%\n",
    "Overall: 10269/15000 = 68.46%\n",
    "Epoch 9\n",
    "Loss: 0.67838 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3935/5053 = 77.8745299822%\n",
    "  Target 1: 3661/4969 = 73.676796136%\n",
    "  Target 2: 2696/4978 = 54.1582965046%\n",
    "Overall: 10292/15000 = 68.6133333333%\n",
    "Epoch 10\n",
    "Loss: 0.67736 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3940/5053 = 77.9734811003%\n",
    "  Target 1: 3591/4969 = 72.2680619843%\n",
    "  Target 2: 2789/4978 = 56.0265166734%\n",
    "Overall: 10320/15000 = 68.8%\n",
    "Epoch 11\n",
    "Loss: 0.67443 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3960/5053 = 78.3692855729%\n",
    "  Target 1: 3672/4969 = 73.8981686456%\n",
    "  Target 2: 2724/4978 = 54.7207713941%\n",
    "Overall: 10356/15000 = 69.04%\n",
    "Epoch 12\n",
    "Loss: 0.67232 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3933/5053 = 77.8349495349%\n",
    "  Target 1: 3602/4969 = 72.4894344939%\n",
    "  Target 2: 2810/4978 = 56.4483728405%\n",
    "Overall: 10345/15000 = 68.9666666667%\n",
    "Epoch 13\n",
    "Loss: 0.67144 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3916/5053 = 77.4985157332%\n",
    "  Target 1: 3508/4969 = 70.5977057758%\n",
    "  Target 2: 2922/4978 = 58.6982723986%\n",
    "Overall: 10346/15000 = 68.9733333333%\n",
    "Epoch 14\n",
    "Loss: 0.67087 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3910/5053 = 77.3797743915%\n",
    "  Target 1: 3563/4969 = 71.7045683236%\n",
    "  Target 2: 2888/4978 = 58.0152671756%\n",
    "Overall: 10361/15000 = 69.0733333333%\n",
    "Epoch 15\n",
    "Loss: 0.66962 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3911/5053 = 77.3995646151%\n",
    "  Target 1: 3443/4969 = 69.2895954921%\n",
    "  Target 2: 3004/4978 = 60.3455202893%\n",
    "Overall: 10358/15000 = 69.0533333333%\n",
    "Epoch 16\n",
    "Loss: 0.66695 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3906/5053 = 77.3006134969%\n",
    "  Target 1: 3017/4969 = 60.71644194%\n",
    "  Target 2: 3283/4978 = 65.9501807955%\n",
    "Overall: 10206/15000 = 68.04%\n",
    "Epoch 17\n",
    "Loss: 0.66658 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3910/5053 = 77.3797743915%\n",
    "  Target 1: 3334/4969 = 67.0959951701%\n",
    "  Target 2: 3100/4978 = 62.2740056247%\n",
    "Overall: 10344/15000 = 68.96%\n",
    "Epoch 18\n",
    "Loss: 0.66540 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3907/5053 = 77.3204037206%\n",
    "  Target 1: 3439/4969 = 69.2090963977%\n",
    "  Target 2: 3033/4978 = 60.9280835677%\n",
    "Overall: 10379/15000 = 69.1933333333%\n",
    "Epoch 19\n",
    "Loss: 0.66394 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3907/5053 = 77.3204037206%\n",
    "  Target 1: 3291/4969 = 66.2306299054%\n",
    "  Target 2: 3133/4978 = 62.9369224588%\n",
    "Overall: 10331/15000 = 68.8733333333%\n",
    "Epoch 20\n",
    "Loss: 0.66208 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3905/5053 = 77.2808232733%\n",
    "  Target 1: 3366/4969 = 67.7399879251%\n",
    "  Target 2: 3083/4978 = 61.9325030133%\n",
    "Overall: 10354/15000 = 69.0266666667%\n",
    "Epoch 21\n",
    "Loss: 0.66210 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3937/5053 = 77.9141104294%\n",
    "  Target 1: 4107/4969 = 82.65244516%\n",
    "  Target 2: 2453/4978 = 49.2768179992%\n",
    "Overall: 10497/15000 = 69.98%\n",
    "Epoch 22\n",
    "Loss: 0.65901 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3931/5053 = 77.7953690877%\n",
    "  Target 1: 3674/4969 = 73.9384181928%\n",
    "  Target 2: 2829/4978 = 56.8300522298%\n",
    "Overall: 10434/15000 = 69.56%\n",
    "Epoch 23\n",
    "Loss: 0.65805 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3931/5053 = 77.7953690877%\n",
    "  Target 1: 3392/4969 = 68.2632320386%\n",
    "  Target 2: 3041/4978 = 61.088790679%\n",
    "Overall: 10364/15000 = 69.0933333333%\n",
    "Epoch 24\n",
    "Loss: 0.65703 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3929/5053 = 77.7557886404%\n",
    "  Target 1: 3592/4969 = 72.2881867579%\n",
    "  Target 2: 2928/4978 = 58.818802732%\n",
    "Overall: 10449/15000 = 69.66%\n",
    "Epoch 25\n",
    "Loss: 0.65604 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3935/5053 = 77.8745299822%\n",
    "  Target 1: 3645/4969 = 73.3547997585%\n",
    "  Target 2: 2887/4978 = 57.9951787867%\n",
    "Overall: 10467/15000 = 69.78%\n",
    "Epoch 26\n",
    "Loss: 0.65516 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3937/5053 = 77.9141104294%\n",
    "  Target 1: 3507/4969 = 70.5775810022%\n",
    "  Target 2: 2953/4978 = 59.3210124548%\n",
    "Overall: 10397/15000 = 69.3133333333%\n",
    "Epoch 27\n",
    "Loss: 0.65290 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3929/5053 = 77.7557886404%\n",
    "  Target 1: 3497/4969 = 70.3763332663%\n",
    "  Target 2: 2989/4978 = 60.0441944556%\n",
    "Overall: 10415/15000 = 69.4333333333%\n",
    "Epoch 28\n",
    "Loss: 0.65343 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3929/5053 = 77.7557886404%\n",
    "  Target 1: 3508/4969 = 70.5977057758%\n",
    "  Target 2: 2985/4978 = 59.9638409%\n",
    "Overall: 10422/15000 = 69.48%\n",
    "Epoch 29\n",
    "Loss: 0.65118 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3947/5053 = 78.1120126657%\n",
    "  Target 1: 3779/4969 = 76.0515194204%\n",
    "  Target 2: 2795/4978 = 56.1470470068%\n",
    "Overall: 10521/15000 = 70.14%\n",
    "Epoch 30\n",
    "Loss: 0.64868 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3942/5053 = 78.0130615476%\n",
    "  Target 1: 3889/4969 = 78.265244516%\n",
    "  Target 2: 2708/4978 = 54.3993571716%\n",
    "Overall: 10539/15000 = 70.26%\n",
    "Epoch 31\n",
    "Loss: 0.64871 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3967/5053 = 78.5078171383%\n",
    "  Target 1: 3797/4969 = 76.4137653451%\n",
    "  Target 2: 2776/4978 = 55.7653676175%\n",
    "Overall: 10540/15000 = 70.2666666667%\n",
    "Epoch 32\n",
    "Loss: 0.64772 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3971/5053 = 78.5869780329%\n",
    "  Target 1: 3871/4969 = 77.9029985913%\n",
    "  Target 2: 2712/4978 = 54.4797107272%\n",
    "Overall: 10554/15000 = 70.36%\n",
    "Epoch 33\n",
    "Loss: 0.64692 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3950/5053 = 78.1713833366%\n",
    "  Target 1: 3875/4969 = 77.9834976857%\n",
    "  Target 2: 2713/4978 = 54.4997991161%\n",
    "Overall: 10538/15000 = 70.2533333333%\n",
    "Epoch 34\n",
    "Loss: 0.64440 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3978/5053 = 78.7255095983%\n",
    "  Target 1: 3951/4969 = 79.512980479%\n",
    "  Target 2: 2641/4978 = 53.0534351145%\n",
    "Overall: 10570/15000 = 70.4666666667%\n",
    "Epoch 35\n",
    "Loss: 0.64478 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3962/5053 = 78.4088660202%\n",
    "  Target 1: 4019/4969 = 80.8814650835%\n",
    "  Target 2: 2583/4978 = 51.8883085577%\n",
    "Overall: 10564/15000 = 70.4266666667%\n",
    "Epoch 36\n",
    "Loss: 0.64443 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3977/5053 = 78.7057193746%\n",
    "  Target 1: 4094/4969 = 82.3908231032%\n",
    "  Target 2: 2500/4978 = 50.220972278%\n",
    "Overall: 10571/15000 = 70.4733333333%\n",
    "Epoch 37\n",
    "Loss: 0.64556 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3971/5053 = 78.5869780329%\n",
    "  Target 1: 4015/4969 = 80.8009659891%\n",
    "  Target 2: 2577/4978 = 51.7677782242%\n",
    "Overall: 10563/15000 = 70.42%\n",
    "Epoch 38\n",
    "Loss: 0.64328 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3962/5053 = 78.4088660202%\n",
    "  Target 1: 4031/4969 = 81.1229623667%\n",
    "  Target 2: 2571/4978 = 51.6472478907%\n",
    "Overall: 10564/15000 = 70.4266666667%\n",
    "Epoch 39\n",
    "Loss: 0.64018 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3968/5053 = 78.527607362%\n",
    "  Target 1: 3873/4969 = 77.9432481385%\n",
    "  Target 2: 2674/4978 = 53.7163519486%\n",
    "Overall: 10515/15000 = 70.1%\n",
    "Epoch 40\n",
    "Loss: 0.64270 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3952/5053 = 78.2109637839%\n",
    "  Target 1: 3723/4969 = 74.924532099%\n",
    "  Target 2: 2813/4978 = 56.5086380072%\n",
    "Overall: 10488/15000 = 69.92%\n",
    "Epoch 41\n",
    "Loss: 0.64048 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3939/5053 = 77.9536908767%\n",
    "  Target 1: 3952/4969 = 79.5331052526%\n",
    "  Target 2: 2669/4978 = 53.615910004%\n",
    "Overall: 10560/15000 = 70.4%\n",
    "Epoch 42\n",
    "Loss: 0.64074 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3957/5053 = 78.309914902%\n",
    "  Target 1: 3918/4969 = 78.8488629503%\n",
    "  Target 2: 2690/4978 = 54.0377661712%\n",
    "Overall: 10565/15000 = 70.4333333333%\n",
    "Epoch 43\n",
    "Loss: 0.64003 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3959/5053 = 78.3494953493%\n",
    "  Target 1: 3906/4969 = 78.6073656671%\n",
    "  Target 2: 2705/4978 = 54.3390920048%\n",
    "Overall: 10570/15000 = 70.4666666667%\n",
    "Epoch 44\n",
    "Loss: 0.63898 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3965/5053 = 78.4682366911%\n",
    "  Target 1: 4042/4969 = 81.3443348762%\n",
    "  Target 2: 2593/4978 = 52.0891924468%\n",
    "Overall: 10600/15000 = 70.6666666667%\n",
    "Epoch 45\n",
    "Loss: 0.64070 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3961/5053 = 78.3890757966%\n",
    "  Target 1: 4148/4969 = 83.4775608774%\n",
    "  Target 2: 2528/4978 = 50.7834471675%\n",
    "Overall: 10637/15000 = 70.9133333333%\n",
    "Epoch 46\n",
    "Loss: 0.63733 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3961/5053 = 78.3890757966%\n",
    "  Target 1: 4061/4969 = 81.7267055746%\n",
    "  Target 2: 2587/4978 = 51.9686621133%\n",
    "Overall: 10609/15000 = 70.7266666667%\n",
    "Epoch 47\n",
    "Loss: 0.63820 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3937/5053 = 77.9141104294%\n",
    "  Target 1: 4066/4969 = 81.8273294425%\n",
    "  Target 2: 2598/4978 = 52.1896343913%\n",
    "Overall: 10601/15000 = 70.6733333333%\n",
    "Epoch 48\n",
    "Loss: 0.63690 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3948/5053 = 78.1318028894%\n",
    "  Target 1: 3973/4969 = 79.9557254981%\n",
    "  Target 2: 2687/4978 = 53.9775010044%\n",
    "Overall: 10608/15000 = 70.72%\n",
    "Epoch 49\n",
    "Loss: 0.63605 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3954/5053 = 78.2505442311%\n",
    "  Target 1: 3935/4969 = 79.1909841014%\n",
    "  Target 2: 2727/4978 = 54.7810365609%\n",
    "Overall: 10616/15000 = 70.7733333333%\n",
    "Epoch 50\n",
    "Loss: 0.63551 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3921/5053 = 77.5974668514%\n",
    "  Target 1: 3953/4969 = 79.5532300262%\n",
    "  Target 2: 2720/4978 = 54.6404178385%\n",
    "Overall: 10594/15000 = 70.6266666667%\n",
    "Epoch 51\n",
    "Loss: 0.63559 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3950/5053 = 78.1713833366%\n",
    "  Target 1: 4031/4969 = 81.1229623667%\n",
    "  Target 2: 2626/4978 = 52.7521092808%\n",
    "Overall: 10607/15000 = 70.7133333333%\n",
    "Epoch 52\n",
    "Loss: 0.63489 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3952/5053 = 78.2109637839%\n",
    "  Target 1: 4048/4969 = 81.4650835178%\n",
    "  Target 2: 2622/4978 = 52.6717557252%\n",
    "Overall: 10622/15000 = 70.8133333333%\n",
    "Epoch 53\n",
    "Loss: 0.63500 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3942/5053 = 78.0130615476%\n",
    "  Target 1: 4063/4969 = 81.7669551218%\n",
    "  Target 2: 2611/4978 = 52.4507834472%\n",
    "Overall: 10616/15000 = 70.7733333333%\n",
    "Epoch 54\n",
    "Loss: 0.63492 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3934/5053 = 77.8547397586%\n",
    "  Target 1: 4064/4969 = 81.7870798954%\n",
    "  Target 2: 2610/4978 = 52.4306950583%\n",
    "Overall: 10608/15000 = 70.72%\n",
    "Epoch 55\n",
    "Loss: 0.63439 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3956/5053 = 78.2901246784%\n",
    "  Target 1: 3967/4969 = 79.8349768565%\n",
    "  Target 2: 2692/4978 = 54.077942949%\n",
    "Overall: 10615/15000 = 70.7666666667%\n",
    "Epoch 56\n",
    "Loss: 0.63639 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3957/5053 = 78.309914902%\n",
    "  Target 1: 4084/4969 = 82.1895753673%\n",
    "  Target 2: 2584/4978 = 51.9083969466%\n",
    "Overall: 10625/15000 = 70.8333333333%\n",
    "Epoch 57\n",
    "Loss: 0.63507 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3971/5053 = 78.5869780329%\n",
    "  Target 1: 4094/4969 = 82.3908231032%\n",
    "  Target 2: 2552/4978 = 51.2655685014%\n",
    "Overall: 10617/15000 = 70.78%\n",
    "Epoch 58\n",
    "Loss: 0.63748 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3959/5053 = 78.3494953493%\n",
    "  Target 1: 4048/4969 = 81.4650835178%\n",
    "  Target 2: 2569/4978 = 51.6070711129%\n",
    "Overall: 10576/15000 = 70.5066666667%\n",
    "Epoch 59\n",
    "Loss: 0.63526 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3961/5053 = 78.3890757966%\n",
    "  Target 1: 4190/4969 = 84.3228013685%\n",
    "  Target 2: 2485/4978 = 49.9196464444%\n",
    "Overall: 10636/15000 = 70.9066666667%\n",
    "Epoch 60\n",
    "Loss: 0.63411 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3951/5053 = 78.1911735603%\n",
    "  Target 1: 4034/4969 = 81.1833366875%\n",
    "  Target 2: 2626/4978 = 52.7521092808%\n",
    "Overall: 10611/15000 = 70.74%\n",
    "Epoch 61\n",
    "Loss: 0.63818 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3952/5053 = 78.2109637839%\n",
    "  Target 1: 4078/4969 = 82.0688267257%\n",
    "  Target 2: 2580/4978 = 51.8280433909%\n",
    "Overall: 10610/15000 = 70.7333333333%\n",
    "Epoch 62\n",
    "Loss: 0.63482 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3948/5053 = 78.1318028894%\n",
    "  Target 1: 4127/4969 = 83.0549406319%\n",
    "  Target 2: 2556/4978 = 51.3459220571%\n",
    "Overall: 10631/15000 = 70.8733333333%\n",
    "Epoch 63\n",
    "Loss: 0.63787 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3959/5053 = 78.3494953493%\n",
    "  Target 1: 4111/4969 = 82.7329442544%\n",
    "  Target 2: 2563/4978 = 51.4865407794%\n",
    "Overall: 10633/15000 = 70.8866666667%\n",
    "Epoch 64\n",
    "Loss: 0.63494 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3947/5053 = 78.1120126657%\n",
    "  Target 1: 3971/4969 = 79.9154759509%\n",
    "  Target 2: 2675/4978 = 53.7364403375%\n",
    "Overall: 10593/15000 = 70.62%\n",
    "Epoch 65\n",
    "Loss: 0.63771 [135000/135000]  |####################| 100.0% \n",
    "  Target 0: 3921/5053 = 77.5974668514%\n",
    "  Target 1: 4197/4969 = 84.4636747837%\n",
    "  Target 2: 2514/4978 = 50.5022097228%\n",
    "Overall: 10632/15000 = 70.88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7feb20347350>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdEAAAHMCAYAAACKi5LeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb327717d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(5,5), dpi = 200)\n",
    "plt.plot(acc_vals[:45])\n",
    "sns.set()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary = \n",
    "Out[22]:\n",
    "array([ 72.5 ,  73.71,  75.01,  76.08,  78.39,  81.06,  83.22,  84.23,\n",
    "        85.14,  85.72,  86.04,  86.23,  86.24,  86.3 ,  86.27,  86.17,\n",
    "        86.58,  86.34,  86.47,  86.77,  86.73,  86.84,  86.66,  86.77,\n",
    "        86.72,  86.92,  86.84,  86.95,  86.9 ,  87.08,  87.1 ,  87.02,\n",
    "        86.91,  86.98,  87.21,  87.27,  87.45,  87.27,  87.51,  87.34,\n",
    "        87.44,  87.56,  87.69,  87.39,  87.65,  87.56,  87.67,  87.5 ,\n",
    "        87.67,  87.67,  87.79,  87.73,  87.87,  87.74,  87.7 ,  87.66,\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
