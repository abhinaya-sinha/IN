{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd.variable import *\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import util\n",
    "from __future__ import print_function\n",
    "import setGPU\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']=\"6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N = 100\n",
    "save_path = '/nfshome/emoreno/IN/'\n",
    "training_pivot = np.load(save_path + 'train_withSpectator_features_0.npy') #per jet constituents\n",
    "training_pivot = np.swapaxes(training_pivot, 1, 2)\n",
    "#training_0 = np.load(save_path + 'train_withSpectator_features_0.npy') #per jet constituents\n",
    "#training_0 = np.swapaxes(training_0, 1, 2)\n",
    "#training_1 = np.load(save_path + 'train_features_1.npy') #10 features of 100 inclusive (charged and neutral) particles\n",
    "#training_1 = np.swapaxes(training_1, 1, 2)\n",
    "#training_2 = np.load(save_path + 'train_features_2.npy') #30 features of 60 charged particles\n",
    "#training_2 = np.swapaxes(training_2, 1, 2)\n",
    "#training_3 = np.load(save_path + 'train_features_3.npy') #14 features of 5 secondary vertices\n",
    "#training_3 = np.swapaxes(training_3, 1, 2)\n",
    "#target = np.load(save_path + 'train_truth_0.npy')\n",
    "#test_pivot = np.load(save_path + 'test_withSpectator_features_0.npy') #per jet constituents\n",
    "#test_pivot = np.swapaxes(test_pivot, 1, 2)\n",
    "#test_0 = np.load(save_path + 'test_features_0.npy')\n",
    "#test_0 = np.swapaxes(test_0, 1, 2)\n",
    "#test_1 = np.load(save_path + 'test_features_1.npy')\n",
    "#test_1 = np.swapaxes(test_1, 1, 2)\n",
    "test_2 = np.load(save_path + 'test_features_2.npy')\n",
    "test_2 = np.swapaxes(test_2, 1, 2)\n",
    "test_3 = np.load(save_path + 'test_features_3.npy')\n",
    "test_3 = np.swapaxes(test_3, 1, 2)\n",
    "target_test = np.load(save_path + 'test_truth_0.npy')\n",
    "params_0 = ['fj_jetNTracks',\n",
    "          'fj_nSV',\n",
    "          'fj_tau0_trackEtaRel_0',\n",
    "          'fj_tau0_trackEtaRel_1',\n",
    "          'fj_tau0_trackEtaRel_2',\n",
    "          'fj_tau1_trackEtaRel_0',\n",
    "          'fj_tau1_trackEtaRel_1',\n",
    "          'fj_tau1_trackEtaRel_2',\n",
    "          'fj_tau_flightDistance2dSig_0',\n",
    "          'fj_tau_flightDistance2dSig_1',\n",
    "          'fj_tau_vertexDeltaR_0',\n",
    "          'fj_tau_vertexEnergyRatio_0',\n",
    "          'fj_tau_vertexEnergyRatio_1',\n",
    "          'fj_tau_vertexMass_0',\n",
    "          'fj_tau_vertexMass_1',\n",
    "          'fj_trackSip2dSigAboveBottom_0',\n",
    "          'fj_trackSip2dSigAboveBottom_1',\n",
    "          'fj_trackSip2dSigAboveCharm_0',\n",
    "          'fj_trackSipdSig_0',\n",
    "          'fj_trackSipdSig_0_0',\n",
    "          'fj_trackSipdSig_0_1',\n",
    "          'fj_trackSipdSig_1',\n",
    "          'fj_trackSipdSig_1_0',\n",
    "          'fj_trackSipdSig_1_1',\n",
    "          'fj_trackSipdSig_2',\n",
    "          'fj_trackSipdSig_3',\n",
    "          'fj_z_ratio'\n",
    "          ]\n",
    "\n",
    "params_1 = ['pfcand_ptrel',\n",
    "          'pfcand_erel',\n",
    "          'pfcand_phirel',\n",
    "          'pfcand_etarel',\n",
    "          'pfcand_deltaR',\n",
    "          'pfcand_puppiw',\n",
    "          'pfcand_drminsv',\n",
    "          'pfcand_drsubjet1',\n",
    "          'pfcand_drsubjet2',\n",
    "          'pfcand_hcalFrac'\n",
    "         ]\n",
    "\n",
    "params_2 = ['track_ptrel',     \n",
    "          'track_erel',     \n",
    "          'track_phirel',     \n",
    "          'track_etarel',     \n",
    "          'track_deltaR',\n",
    "          'track_drminsv',     \n",
    "          'track_drsubjet1',     \n",
    "          'track_drsubjet2',\n",
    "          'track_dz',     \n",
    "          'track_dzsig',     \n",
    "          'track_dxy',     \n",
    "          'track_dxysig',     \n",
    "          'track_normchi2',     \n",
    "          'track_quality',     \n",
    "          'track_dptdpt',     \n",
    "          'track_detadeta',     \n",
    "          'track_dphidphi',     \n",
    "          'track_dxydxy',     \n",
    "          'track_dzdz',     \n",
    "          'track_dxydz',     \n",
    "          'track_dphidxy',     \n",
    "          'track_dlambdadz',     \n",
    "          'trackBTag_EtaRel',     \n",
    "          'trackBTag_PtRatio',     \n",
    "          'trackBTag_PParRatio',     \n",
    "          'trackBTag_Sip2dVal',     \n",
    "          'trackBTag_Sip2dSig',     \n",
    "          'trackBTag_Sip3dVal',     \n",
    "          'trackBTag_Sip3dSig',     \n",
    "          'trackBTag_JetDistVal'\n",
    "         ]\n",
    "\n",
    "params_3 = ['sv_ptrel',\n",
    "          'sv_erel',\n",
    "          'sv_phirel',\n",
    "          'sv_etarel',\n",
    "          'sv_deltaR',\n",
    "          'sv_pt',\n",
    "          'sv_mass',\n",
    "          'sv_ntracks',\n",
    "          'sv_normchi2',\n",
    "          'sv_dxy',\n",
    "          'sv_dxysig',\n",
    "          'sv_d3d',\n",
    "          'sv_d3dsig',\n",
    "          'sv_costhetasvpv'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/nfshome/emoreno/IN/data/opendata/test/'\n",
    "#test_0 = np.load(save_path + 'test_features_0.npy')\n",
    "#test_0 = np.swapaxes(test_0, 1, 2)\n",
    "#training_0 = np.load(save_path + 'train_withSpectator_features_0.npy') #per jet constituents\n",
    "#training_0 = np.swapaxes(training_0, 1, 2)\n",
    "#training_2 = np.load(save_path + 'train_features_2.npy') #30 features of 60 charged particles\n",
    "#training_2 = np.swapaxes(training_2, 1, 2)\n",
    "#training_3 = np.load(save_path + 'train_features_3.npy') #14 features of 5 secondary vertices\n",
    "#training_3 = np.swapaxes(training_3, 1, 2)\n",
    "#target = np.load(save_path + 'train_truth_0.npy')\n",
    "#spectator = np.load(save_path + 'test_spectator_0.npy')\n",
    "#spectator = np.swapaxes(spectator, 1, 2)\n",
    "#test_0 = np.load(save_path + 'test_0.npy')\n",
    "#test_0 = np.swapaxes(test_0, 1, 2)\n",
    "#print(test_0.shape)\n",
    "#test_1 = np.load(save_path + 'test_features_1.npy')\n",
    "#test_1 = np.swapaxes(test_1, 1, 2)\n",
    "test_2 = np.load(save_path + 'test_2.npy')\n",
    "test_2 = np.swapaxes(test_2, 1, 2)\n",
    "test_3 = np.load(save_path + 'test_3.npy')\n",
    "test_3 = np.swapaxes(test_3, 1, 2)\n",
    "target_test = np.load(save_path + 'truth_0.npy')\n",
    "params_0 = ['fj_jetNTracks',\n",
    "          'fj_nSV',\n",
    "          'fj_tau0_trackEtaRel_0',\n",
    "          'fj_tau0_trackEtaRel_1',\n",
    "          'fj_tau0_trackEtaRel_2',\n",
    "          'fj_tau1_trackEtaRel_0',\n",
    "          'fj_tau1_trackEtaRel_1',\n",
    "          'fj_tau1_trackEtaRel_2',\n",
    "          'fj_tau_flightDistance2dSig_0',\n",
    "          'fj_tau_flightDistance2dSig_1',\n",
    "          'fj_tau_vertexDeltaR_0',\n",
    "          'fj_tau_vertexEnergyRatio_0',\n",
    "          'fj_tau_vertexEnergyRatio_1',\n",
    "          'fj_tau_vertexMass_0',\n",
    "          'fj_tau_vertexMass_1',\n",
    "          'fj_trackSip2dSigAboveBottom_0',\n",
    "          'fj_trackSip2dSigAboveBottom_1',\n",
    "          'fj_trackSip2dSigAboveCharm_0',\n",
    "          'fj_trackSipdSig_0',\n",
    "          'fj_trackSipdSig_0_0',\n",
    "          'fj_trackSipdSig_0_1',\n",
    "          'fj_trackSipdSig_1',\n",
    "          'fj_trackSipdSig_1_0',\n",
    "          'fj_trackSipdSig_1_1',\n",
    "          'fj_trackSipdSig_2',\n",
    "          'fj_trackSipdSig_3',\n",
    "          'fj_z_ratio'\n",
    "          ]\n",
    "\n",
    "params_1 = ['pfcand_ptrel',\n",
    "          'pfcand_erel',\n",
    "          'pfcand_phirel',\n",
    "          'pfcand_etarel',\n",
    "          'pfcand_deltaR',\n",
    "          'pfcand_puppiw',\n",
    "          'pfcand_drminsv',\n",
    "          'pfcand_drsubjet1',\n",
    "          'pfcand_drsubjet2',\n",
    "          'pfcand_hcalFrac'\n",
    "         ]\n",
    "\n",
    "params_2 = ['track_ptrel',     \n",
    "          'track_erel',     \n",
    "          'track_phirel',     \n",
    "          'track_etarel',     \n",
    "          'track_deltaR',\n",
    "          'track_drminsv',     \n",
    "          'track_drsubjet1',     \n",
    "          'track_drsubjet2',\n",
    "          'track_dz',     \n",
    "          'track_dzsig',     \n",
    "          'track_dxy',     \n",
    "          'track_dxysig',     \n",
    "          'track_normchi2',     \n",
    "          'track_quality',     \n",
    "          'track_dptdpt',     \n",
    "          'track_detadeta',     \n",
    "          'track_dphidphi',     \n",
    "          'track_dxydxy',     \n",
    "          'track_dzdz',     \n",
    "          'track_dxydz',     \n",
    "          'track_dphidxy',     \n",
    "          'track_dlambdadz',     \n",
    "          'trackBTag_EtaRel',     \n",
    "          'trackBTag_PtRatio',     \n",
    "          'trackBTag_PParRatio',     \n",
    "          'trackBTag_Sip2dVal',     \n",
    "          'trackBTag_Sip2dSig',     \n",
    "          'trackBTag_Sip3dVal',     \n",
    "          'trackBTag_Sip3dSig',     \n",
    "          'trackBTag_JetDistVal'\n",
    "         ]\n",
    "\n",
    "params_3 = ['sv_ptrel',\n",
    "          'sv_erel',\n",
    "          'sv_phirel',\n",
    "          'sv_etarel',\n",
    "          'sv_deltaR',\n",
    "          'sv_pt',\n",
    "          'sv_mass',\n",
    "          'sv_ntracks',\n",
    "          'sv_normchi2',\n",
    "          'sv_dxy',\n",
    "          'sv_dxysig',\n",
    "          'sv_d3d',\n",
    "          'sv_d3dsig',\n",
    "          'sv_costhetasvpv'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import H5Data\n",
    "\n",
    "files = []\n",
    "file_number = 36\n",
    "for i in range(file_number):\n",
    "    files.append(\"/nfshome/emoreno/IN/data/train_ready/data_\" + str(i))\n",
    "\n",
    "data = H5Data(batch_size = 100000,\n",
    "               cache = None,\n",
    "               preloading=0,\n",
    "               features_name='training_subgroup', labels_name='target_subgroup')\n",
    "data.set_file_names(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import H5Data\n",
    "\n",
    "files = []\n",
    "file_number = 10\n",
    "for i in range(file_number):\n",
    "    files.append(\"/nfshome/emoreno/IN/data/opendata/train/data_\" + str(i))\n",
    "\n",
    "data = H5Data(batch_size = 100000,\n",
    "               cache = None,\n",
    "               preloading=0,\n",
    "               features_name='training_subgroup', labels_name='target_subgroup')\n",
    "data.set_file_names(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "spectator = np.load('/nfshome/emoreno/IN/data/opendata/test/test_spectator_0.npy')\n",
    "spectator = np.swapaxes(spectator, 1, 2)\n",
    "test_pivot = np.array([i[2] for i in spectator])\n",
    "NENT = 1 # take all events\n",
    "NBINS=40 # number of bins for loss function\n",
    "MMAX = 200. # max value\n",
    "MMIN = 40. # min value\n",
    "LAMBDA = 1 # lambda for penalty\n",
    "\n",
    "binWidth = (MMAX - MMIN) / NBINS\n",
    "test_pivot_OHE = np.zeros((test_pivot.shape[0], NBINS))\n",
    "#test_pivot_OHE = np.zeros((test_pivot.shape[0], NBINS))\n",
    "\n",
    "for i in range(0,test_pivot.shape[0]):\n",
    "    if test_pivot[i][0] > MMAX: \n",
    "        test_pivot_OHE[i, -1] = 1\n",
    "    elif test_pivot[i][0] <= MMIN: \n",
    "        test_pivot_OHE[i, 0] = 1\n",
    "    else:\n",
    "        test_pivot_OHE[i,int((test_pivot[i][0]-MMIN)/binWidth)] = 1\n",
    "\n",
    "del test_pivot\n",
    "test_pivot = test_pivot_OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert two sets into two branch with one set in both and one set in only one (Use for this file)\n",
    "\n",
    "#training = training_2\n",
    "test = test_2\n",
    "params = params_2\n",
    "#training_sv = training_3\n",
    "test_sv = test_3\n",
    "params_sv = params_3\n",
    "N = test.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predict, target):\n",
    "    _, p_vals = torch.max(predict, 1)\n",
    "    r = torch.sum(target == p_vals.squeeze(1)).data.numpy()[0]\n",
    "    t = target.size()[0]\n",
    "    return r * 1.0 / t\n",
    "\n",
    "def stats(predict, target):\n",
    "    _, p_vals = torch.max(predict, 1)\n",
    "    t = target.cpu().data.numpy()\n",
    "    p_vals = p_vals.squeeze(0).cpu().data.numpy()\n",
    "    vals = np.unique(t)\n",
    "    for i in vals:\n",
    "        ind = np.where(t == i)\n",
    "        pv = p_vals[ind]\n",
    "        correct = sum(pv == t[ind])\n",
    "        print(\"  Target %s: %s/%s = %s%%\" % (i, correct, len(pv), correct * 100.0/len(pv)))\n",
    "    print(\"Overall: %s/%s = %s%%\" % (sum(p_vals == t), len(t), sum(p_vals == t) * 100.0/len(t)))\n",
    "    return sum(p_vals == t) * 100.0/len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "def get_cmap(n, name='hsv'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n + 1)\n",
    "\n",
    "def predicted_histogram(data, \n",
    "                        target, \n",
    "                        labels = None, \n",
    "                        nbins = 10, \n",
    "                        out = None,\n",
    "                        xlabel = None,\n",
    "                        title = None\n",
    "                       ):\n",
    "    \"\"\"@params:\n",
    "        data = n x 1 array of parameter values\n",
    "        target = n x categories array of predictions\n",
    "    \"\"\"\n",
    "    target = preprocessing.normalize(target, norm = \"l1\")\n",
    "    if labels == None:\n",
    "        labels = [\"\" for i in range(target.shape[1])]\n",
    "    #1 decide bins\n",
    "    ma = np.amax(data) * 1.0\n",
    "    mi = np.amin(data)\n",
    "    bins = np.linspace(mi, ma, nbins)\n",
    "    bin_size = bins[1] - bins[0]\n",
    "    bin_locs = np.digitize(data, bins, right = True)\n",
    "    #2 set up bin x category matrix\n",
    "    #  Each M(bin, category) = Sum over particles with param in bin of category\n",
    "    M = np.array([np.sum(target[np.where(bin_locs == i)], axis = 0) \n",
    "                  for i in range(nbins)])\n",
    "    #3 plot each category/bin\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "    bars = np.array([M[:, i] for i in range(M.shape[1])])\n",
    "    cmap = get_cmap(len(bars), 'viridis')\n",
    "    for i in range(len(bars)):\n",
    "        ax.bar(bins, bars[i], \n",
    "               bottom = sum(bars[:i]), \n",
    "               color = cmap(i), \n",
    "               label = labels[i],\n",
    "               width = bin_size\n",
    "              )\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "def generate_control_plots():\n",
    "    #global gnn\n",
    "    len_params = len(params)\n",
    "    path = '/nfshome/emoreno/IN/img/n-h-hb/'\n",
    "    #os.makedirs(path)\n",
    "    fr = 0\n",
    "    b = 1000\n",
    "    pred= None\n",
    "    while fr< valv.shape[0]: #beginning splitting up valv into batches because memory runs out\n",
    "        print (\"Predicting from\",fr)\n",
    "        valv_1 = valv[fr:fr+b,...]\n",
    "        p = gnn(valv_1.cuda())\n",
    "        valv_1.cpu()\n",
    "        p = p.cpu().data\n",
    "        fr +=b\n",
    "        if pred is None:\n",
    "            pred = p\n",
    "        else:\n",
    "            pred = np.append(pred,p,axis=0)\n",
    "        print (pred.shape) #end \n",
    "\n",
    "    d_target = np.array([util.get_list_from_num(i, length = n_targets) \n",
    "                             for i in val_targetv.cpu().data.numpy()])\n",
    "    p_target = pred#.cpu().data.numpy()\n",
    "    for i in range(len(params)):\n",
    "        xlabel = params[i]\n",
    "        labels = [\"None\", \"H\", \"H + b\"]\n",
    "        data = np.mean(valv.data.numpy()[:, i, :], axis = 1)\n",
    "        predicted_histogram(data, d_target, \n",
    "                            nbins = 50, labels = labels,\n",
    "                            xlabel = xlabel, \n",
    "                            title = \"Actual Distribution\"\n",
    "                           )\n",
    "        plt.savefig(path + xlabel + \"-actual.png\", dpi = 200)\n",
    "        predicted_histogram(data, p_target, \n",
    "                            nbins = 50, labels = labels,\n",
    "                            xlabel = xlabel,\n",
    "                            title = \"Predicted Distribution\"\n",
    "                           )\n",
    "        plt.savefig(path + xlabel + \"-predicted.png\", dpi = 200)\n",
    "        plt.close(\"all\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn import utils\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "def get_sample(training1, training2, training3, target, choice):\n",
    "    target_vals = np.argmax(target, axis = 1)\n",
    "    ind, = np.where(target_vals == choice)\n",
    "    chosen_ind = np.random.choice(ind, 300000)\n",
    "    return training1[chosen_ind], training2[chosen_ind], training3[chosen_ind], target[chosen_ind]\n",
    "\n",
    "def get_sample_train(training1, training2, training3, target, choice):\n",
    "    target_vals = np.argmax(target, axis = 1)\n",
    "    ind, = np.where(target_vals == choice)\n",
    "    chosen_ind = ind\n",
    "    #chosen_ind = np.random.choice(ind, 200000)\n",
    "    return training1[chosen_ind], training2[chosen_ind], training3[chosen_ind], target[chosen_ind]\n",
    "\n",
    "# Training Set\n",
    "\n",
    "val_split = 0.1\n",
    "batch_size =128\n",
    "n_epochs = 250\n",
    "\n",
    "#Test Set\n",
    "n_targets_test = target_test.shape[1]\n",
    "samples_test = [get_sample(test, test_pivot, test_sv, target_test, i) for i in range(n_targets_test)]\n",
    "tests = [i[0] for i in samples_test]\n",
    "tests_pivot = [i[1] for i in samples_test]\n",
    "tests_sv = [i[2] for i in samples_test]\n",
    "targets_tests = [i[3] for i in samples_test]\n",
    "big_test = np.concatenate(tests)\n",
    "big_test_pivot = np.concatenate(tests_pivot)\n",
    "big_test_sv = np.concatenate(tests_sv)\n",
    "big_target_test = np.concatenate(targets_tests)\n",
    "big_test, big_test_pivot, big_test_sv, big_target_test = utils.shuffle(big_test, big_test_pivot, big_test_sv, big_target_test)\n",
    "\n",
    "testv = Variable(torch.FloatTensor(big_test))\n",
    "testv_pivot = Variable(torch.FloatTensor(big_test_pivot))\n",
    "testv_sv = Variable(torch.FloatTensor(big_test_sv))\n",
    "targetv_test = Variable(torch.from_numpy(np.argmax(big_target_test, axis = 1)).long())  \n",
    "testv, valv_test = torch.split(testv, int(testv.size()[0] * (1 - val_split)))\n",
    "testv_pivot, valv_test_pivot = torch.split(testv_pivot, int(testv_pivot.size()[0] * (1 - val_split)))\n",
    "testv_sv, valv_test_sv = torch.split(testv_sv, int(testv_sv.size()[0] * (1 - val_split)))\n",
    "targetv_test, val_targetv_test = torch.split(targetv_test, int(targetv_test.size()[0] * (1 - val_split)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax()\n",
    "\n",
    "class Rx(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Rx, self).__init__()\n",
    "        self.dense1 = nn.Linear(6, 64).cuda()\n",
    "        self.dense2 = nn.Linear(64, 64).cuda()\n",
    "        self.dense3 = nn.Linear(64, 40).cuda()\n",
    "        \n",
    "    def forward(self, x, y, z):\n",
    "        out = gnn(x, y, z)\n",
    "        n = nn.functional.relu(self.dense1(out[1]))\n",
    "        n = nn.functional.relu(self.dense2(n))\n",
    "        n = nn.functional.relu(self.dense2(n))\n",
    "        n = nn.functional.relu(self.dense2(n))\n",
    "        n = nn.functional.softmax(self.dense3(n), dim = 1)\n",
    "        return n\n",
    "        \n",
    "\n",
    "loss_D = nn.BCELoss()\n",
    "   \n",
    "loss_R = nn.CrossEntropyLoss()\n",
    "    \n",
    "DfR = Rx()\n",
    "#DfR.load_state_dict(torch.load('IN_opendata_adversarial_adversary_V3'))\n",
    "opt_DfR = optim.SGD(DfR.parameters(), momentum = 0, lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn import utils\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "class GraphNet(nn.Module):\n",
    "    def __init__(self, n_constituents, n_targets, params, hidden):\n",
    "        super(GraphNet, self).__init__()\n",
    "        self.hidden = hidden\n",
    "        self.P = len(params)\n",
    "        self.N = n_constituents\n",
    "        self.S = test_sv.shape[1]\n",
    "        self.Nv = test_sv.shape[2]\n",
    "        self.Nr = self.N * (self.N - 1)\n",
    "        self.Dr = 0\n",
    "        self.De = 5\n",
    "        self.Dx = 0\n",
    "        self.Do = 6\n",
    "        self.n_targets = n_targets\n",
    "        self.assign_matrices()\n",
    "        self.assign_matrices_SV()\n",
    "        #self.switch = switch\n",
    "        \n",
    "        self.Ra = torch.ones(self.Dr, self.Nr)\n",
    "        self.fr1 = nn.Linear(2 * self.P + self.Dr, hidden).cuda()\n",
    "        self.fr1_sv = nn.Linear(self.S + self.P + self.Dr, hidden).cuda()\n",
    "        self.fr2 = nn.Linear(hidden, hidden/2).cuda()\n",
    "        self.fr3 = nn.Linear(hidden/2, self.De).cuda()\n",
    "        self.fo1 = nn.Linear(self.P + self.Dx + (2 * self.De), hidden).cuda()\n",
    "        self.fo2 = nn.Linear(hidden, hidden/2).cuda()\n",
    "        self.fo3 = nn.Linear(hidden/2, self.Do).cuda()\n",
    "        self.fc1 = nn.Linear(self.Do * self.N, hidden).cuda()\n",
    "        self.fc2 = nn.Linear(hidden, hidden/2).cuda()\n",
    "        self.fc3 = nn.Linear(hidden/2, self.n_targets).cuda()\n",
    "        self.fc_fixed = nn.Linear(self.Do, self.n_targets).cuda()\n",
    "        #self.gru = nn.GRU(input_size = self.Do, hidden_size = 20, bidirectional = False).cuda()\n",
    "            \n",
    "    def assign_matrices(self):\n",
    "        self.Rr = torch.zeros(self.N, self.Nr)\n",
    "        self.Rs = torch.zeros(self.N, self.Nr)\n",
    "        receiver_sender_list = [i for i in itertools.product(range(self.N), range(self.N)) if i[0]!=i[1]]\n",
    "        for i, (r, s) in enumerate(receiver_sender_list):\n",
    "            self.Rr[r, i] = 1\n",
    "            self.Rs[s, i] = 1\n",
    "        self.Rr = (self.Rr).cuda()\n",
    "        self.Rs = (self.Rs).cuda()\n",
    "    \n",
    "    def assign_matrices_SV(self):\n",
    "        self.Rk = torch.zeros(self.N, self.Nr)\n",
    "        self.Rv = torch.zeros(self.Nv, self.Nr)\n",
    "        receiver_sender_list = [i for i in itertools.product(range(self.N), range(self.Nv)) if i[0]!=i[1]]\n",
    "        for i, (k, v) in enumerate(receiver_sender_list):\n",
    "            self.Rk[k, i] = 1\n",
    "            self.Rv[v, i] = 1\n",
    "        self.Rk = (self.Rk).cuda()\n",
    "        self.Rv = (self.Rv).cuda()\n",
    "        \n",
    "    def forward(self, x, y, z):\n",
    "        ###PF Candidate - PF Candidate###\n",
    "        Orr = self.tmul(x, self.Rr)\n",
    "        Ors = self.tmul(x, self.Rs)\n",
    "        B = torch.cat([Orr, Ors], 1)\n",
    "        ### First MLP ###\n",
    "        B = torch.transpose(B, 1, 2).contiguous()\n",
    "        B = nn.functional.relu(self.fr1(B.view(-1, 2 * self.P + self.Dr)))\n",
    "        B = nn.functional.relu(self.fr2(B))\n",
    "        E = nn.functional.relu(self.fr3(B).view(-1, self.Nr, self.De))\n",
    "        del B\n",
    "        E = torch.transpose(E, 1, 2).contiguous()\n",
    "        Ebar = self.tmul(E, torch.transpose(self.Rr, 0, 1).contiguous())\n",
    "        del E\n",
    "        \n",
    "        ####Secondary Vertex - PF Candidate### \n",
    "        Ork = self.tmul(x, self.Rk)\n",
    "        Orv = self.tmul(y, self.Rv)\n",
    "        B = torch.cat([Ork, Orv], 1)\n",
    "        ### First MLP ###\n",
    "        B = torch.transpose(B, 1, 2).contiguous()\n",
    "        B = nn.functional.relu(self.fr1_sv(B.view(-1, self.S + self.P + self.Dr)))\n",
    "        B = nn.functional.relu(self.fr2(B))\n",
    "        E = nn.functional.relu(self.fr3(B).view(-1, self.Nr, self.De))\n",
    "        del B\n",
    "        E = torch.transpose(E, 1, 2).contiguous()\n",
    "        Ebar_sv = self.tmul(E, torch.transpose(self.Rr, 0, 1).contiguous())\n",
    "        del E\n",
    "\n",
    "        ####Final output matrix###\n",
    "        C = torch.cat([x, Ebar], 1)\n",
    "        del Ebar\n",
    "        C = torch.cat([C, Ebar_sv], 1)\n",
    "        del Ebar_sv\n",
    "        C = torch.transpose(C, 1, 2).contiguous()\n",
    "        ### Second MLP ###\n",
    "        C = nn.functional.relu(self.fo1(C.view(-1, self.P + self.Dx + (2 * self.De))))\n",
    "        C = nn.functional.relu(self.fo2(C))\n",
    "        O = nn.functional.relu(self.fo3(C).view(-1, self.N, self.Do))\n",
    "        #Taking the mean/sum of each column\n",
    "        #N = torch.mean(O, dim=1)\n",
    "        N = torch.sum(O, dim=1)\n",
    "        del C\n",
    "        ### Classification MLP ###\n",
    "        #N = nn.functional.relu(self.fc1(O.view(-1, self.Do * self.N)))\n",
    "        del O\n",
    "        #N = nn.functional.relu(self.fc2(N))\n",
    "        #N = nn.functional.relu(self.fc3(N))\n",
    "        F = nn.functional.relu(self.fc_fixed(N))\n",
    "        #P = np.array(N.data.cpu().numpy())\n",
    "        #N = np.zeros((128, 1, 6))\n",
    "        #for i in range(batch_size):\n",
    "        #    N[i] = np.array(np.split(P[i], self.Do))\n",
    "        #    N[1] = [P[i]]\n",
    "        #N, hn = self.gru(torch.tensor(N).cuda())\n",
    "        return (F, N) \n",
    "        \n",
    "            \n",
    "    def tmul(self, x, y):  #Takes (I * J * K)(K * L) -> I * J * L \n",
    "        x_shape = x.size()\n",
    "        y_shape = y.size()\n",
    "        \n",
    "        a = torch.mm(x.view(-1, x_shape[2]), y)\n",
    "        \n",
    "        return torch.mm(x.view(-1, x_shape[2]), y).view(-1, x_shape[1], y_shape[1])\n",
    "\n",
    "# Neural Network\n",
    "n_targets = 2\n",
    "gnn = GraphNet(N, n_targets, params, 15)\n",
    "gnn.load_state_dict(torch.load('IN_opendata_V2'))\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gnn.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_vals_training_adversary = np.zeros(n_epochs)\n",
    "loss_vals_training_classifier = np.zeros(n_epochs)\n",
    "loss_validation_std = np.zeros(n_epochs)\n",
    "loss_training_std = np.zeros(n_epochs)\n",
    "loss_vals_validation_adversary = np.zeros(n_epochs)\n",
    "loss_vals_validation_classifier = np.zeros(n_epochs)\n",
    "acc_vals = np.zeros(n_epochs)\n",
    "final_epoch = 0\n",
    "lam = 100\n",
    "\n",
    "# D Pretraining\n",
    "for m in range(0):\n",
    "    print(\"Epoch %s\" % m)\n",
    "    gnn = gnn.train()\n",
    "    lst = []\n",
    "    loss_val = []\n",
    "    loss_training = []\n",
    "    correct = []\n",
    "    torch.cuda.empty_cache()\n",
    "    final_epoch += 1\n",
    "    step = 0 \n",
    "    \n",
    "    for sub_X,sub_Y in data.generate_data():\n",
    "        \n",
    "        big_training = sub_X[3]\n",
    "        big_training_pivot = sub_X[0]\n",
    "        big_training_sv = sub_X[4]\n",
    "        big_target = sub_Y[0]\n",
    "        \n",
    "        trainingv = Variable(torch.FloatTensor(big_training))\n",
    "        trainingv_pivot = Variable(torch.from_numpy(np.argmax(big_training_pivot, axis = 1)).long())\n",
    "        trainingv_sv = Variable(torch.FloatTensor(big_training_sv))\n",
    "        targetv = Variable(torch.from_numpy(np.argmax(big_target, axis = 1)).long()) \n",
    "        trainingv, valv = torch.split(trainingv, int(trainingv.size()[0] * (1 - val_split)))\n",
    "        trainingv_pivot, valv_pivot = torch.split(trainingv_pivot, int(trainingv_pivot.size()[0] * (1 - val_split)))\n",
    "        trainingv_sv, valv_sv = torch.split(trainingv_sv, int(trainingv_sv.size()[0] * (1 - val_split)))\n",
    "        targetv, val_targetv = torch.split(targetv, int(targetv.size()[0] * (1 - val_split)))\n",
    "        samples_random = np.random.choice(range(len(trainingv)), valv.size()[0]/100)\n",
    "        \n",
    "        print('\\nStep: ' + str(step) + '/' + str(file_number))\n",
    "        step += 1\n",
    "        \n",
    "        for j in range(0, trainingv.size()[0], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            out = gnn(trainingv[j:j + batch_size].cuda(), trainingv_sv[j:j + batch_size].cuda(), trainingv_pivot[j:j + batch_size].cuda())\n",
    "            l = loss(out[0], targetv[j:j + batch_size].cuda())\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            loss_string = \"Loss: %s\" % \"{0:.5f}\".format(l.item())\n",
    "            util.printProgressBar(j + batch_size, trainingv.size()[0], \n",
    "                                  prefix = \"%s [%s/%s] \" % (loss_string, \n",
    "                                                           j + batch_size, \n",
    "                                                           trainingv.size()[0]),\n",
    "                                  length = 20)\n",
    "    \n",
    "    gnn = gnn.eval()\n",
    "    \n",
    "    for sub_X,sub_Y in data.generate_data():\n",
    "        \n",
    "        big_training = sub_X[3]\n",
    "        big_training_pivot = sub_X[0]\n",
    "        big_training_sv = sub_X[4]\n",
    "        big_target = sub_Y[0]\n",
    "\n",
    "        trainingv = Variable(torch.FloatTensor(big_training))\n",
    "        trainingv_pivot = Variable(torch.from_numpy(np.argmax(big_training_pivot, axis = 1)).long())\n",
    "        trainingv_sv = Variable(torch.FloatTensor(big_training_sv))\n",
    "        targetv = Variable(torch.from_numpy(np.argmax(big_target, axis = 1)).long()) \n",
    "        trainingv, valv = torch.split(trainingv, int(trainingv.size()[0] * (1 - val_split)))\n",
    "        trainingv_pivot, valv_pivot = torch.split(trainingv_pivot, int(trainingv_pivot.size()[0] * (1 - val_split)))\n",
    "        trainingv_sv, valv_sv = torch.split(trainingv_sv, int(trainingv_sv.size()[0] * (1 - val_split)))\n",
    "        targetv, val_targetv = torch.split(targetv, int(targetv.size()[0] * (1 - val_split)))\n",
    "        samples_random = np.random.choice(range(len(trainingv)), valv.size()[0]/100)\n",
    "        \n",
    "        # Validation Loss\n",
    "\n",
    "        for j in range(0, valv.size()[0], batch_size):\n",
    "            out = gnn(valv[j:j + batch_size].cuda(), valv_sv[j:j + batch_size].cuda(), valv_pivot[j:j + batch_size].cuda())\n",
    "            lst.append(out[0].cpu().data.numpy())\n",
    "            l_val = loss(out[0], val_targetv[j:j + batch_size].cuda())\n",
    "            loss_val.append(l_val.item())\n",
    "\n",
    "        val_targetv_cpu = val_targetv.cpu().data.numpy()\n",
    "        for n in range(val_targetv_cpu.shape[0]):\n",
    "            correct.append(val_targetv_cpu[n])\n",
    "\n",
    "        # Training Loss\n",
    "\n",
    "        for j in samples_random:\n",
    "            out = gnn(trainingv[j:j + batch_size].cuda(), trainingv_sv[j:j + batch_size].cuda(), trainingv_pivot[j:j + batch_size].cuda())\n",
    "            l_training = loss(out[0], targetv[j:j + batch_size].cuda())\n",
    "            loss_training.append(l_training.item())\n",
    "    \n",
    "        \n",
    "    l_val = np.mean(np.array(loss_val))\n",
    "    predicted = (torch.FloatTensor(np.concatenate(lst))).to(device)\n",
    "    print('\\nValidation Loss: ', l_val)\n",
    "\n",
    "    l_training = np.mean(np.array(loss_training))\n",
    "    print('Training Loss: ', l_training)\n",
    "    val_targetv = torch.FloatTensor(np.array(correct)).cuda()\n",
    "    \n",
    "    torch.save(gnn.state_dict(), 'IN_opendata_adversarial_classifier_lambda100_V2')\n",
    "    acc_vals[m] = stats(predicted, val_targetv)\n",
    "    loss_vals_training[m] = l_training\n",
    "    loss_vals_validation[m] = l_val\n",
    "    loss_validation_std[m] = np.std(np.array(loss_val))\n",
    "    loss_training_std[m] = np.std(np.array(loss_training))\n",
    "    \n",
    "# R Pretraining\n",
    "for i in range(10):\n",
    "    print(\"Epoch %s\" % i)\n",
    "    #torch.cuda.empty_cache()\n",
    "    step = 0\n",
    "    c = 1.0\n",
    "    gnn = gnn.eval()\n",
    "    \n",
    "    for sub_X,sub_Y in data.generate_data():\n",
    "        \n",
    "        big_training = sub_X[3]\n",
    "        big_training_pivot = sub_X[0]\n",
    "        big_training_sv = sub_X[4]\n",
    "        big_target = sub_Y[0]\n",
    "        \n",
    "\n",
    "        trainingv = Variable(torch.FloatTensor(big_training))\n",
    "        trainingv_pivot = Variable(torch.from_numpy(np.argmax(big_training_pivot, axis = 1)).long())\n",
    "        trainingv_sv = Variable(torch.FloatTensor(big_training_sv))\n",
    "        targetv = Variable(torch.from_numpy(np.argmax(big_target, axis = 1)).long()) \n",
    "        trainingv, valv = torch.split(trainingv, int(trainingv.size()[0] * (1 - val_split)))\n",
    "        trainingv_pivot, valv_pivot = torch.split(trainingv_pivot, int(trainingv_pivot.size()[0] * (1 - val_split)))\n",
    "        trainingv_sv, valv_sv = torch.split(trainingv_sv, int(trainingv_sv.size()[0] * (1 - val_split)))\n",
    "        targetv, val_targetv = torch.split(targetv, int(targetv.size()[0] * (1 - val_split)))\n",
    "        samples_random = np.random.choice(range(len(trainingv)), valv.size()[0]/100)\n",
    "        \n",
    "        print('\\nStep: ' + str(step) + '/' + str(file_number))\n",
    "        step += 1\n",
    "        \n",
    "        for j in range(0, trainingv.size()[0], batch_size):\n",
    "            opt_DfR.zero_grad()\n",
    "            out = DfR(trainingv[j:j + batch_size].cuda(), trainingv_sv[j:j + batch_size].cuda(), trainingv_pivot[j:j + batch_size].cuda())\n",
    "            l_DfR = loss(out, trainingv_pivot[j:j + batch_size].cuda())\n",
    "            l_DfR.backward()\n",
    "            opt_DfR.step()\n",
    "            loss_string = \"Loss: %s\" % \"{0:.5f}\".format(l_DfR.item())\n",
    "            util.printProgressBar(j + batch_size, trainingv.size()[0], \n",
    "                                  prefix = \"%s [%s/%s] \" % (loss_string, \n",
    "                                                           j + batch_size, \n",
    "                                                           trainingv.size()[0]),\n",
    "                                  length = 20)\n",
    "        print('\\n')\n",
    "\n",
    "    '''\n",
    "    lst = []\n",
    "    loss_val = []\n",
    "    for j in range(0, valv.size()[0], batch_size):\n",
    "        out = DfR(valv[j:j + batch_size].cuda(), valv_sv[j:j + batch_size].cuda(), valv_pivot[j:j + batch_size].cuda())\n",
    "        lst.append(out.cpu().data.numpy())\n",
    "        l_val = loss_R(out, valv_pivot[j:j + batch_size].cuda())\n",
    "        loss_val.append(l_val.item())\n",
    "    l_val = np.mean(np.array(loss_val))\n",
    "    predicted = (torch.FloatTensor(np.concatenate(lst))).to(device)\n",
    "    print('\\nValidation Loss: ', l_val)\n",
    "    '''\n",
    "    \n",
    "\n",
    "for m in range(100):\n",
    "    print(\"Epoch %s\" % m)\n",
    "    #torch.cuda.empty_cache()\n",
    "    final_epoch += 1\n",
    "    DfR = DfR.eval()\n",
    "    gnn = gnn.train()\n",
    "    c = 1\n",
    "    lst = []\n",
    "    loss_val_adversary = []\n",
    "    loss_val_classifier = []\n",
    "    loss_training_adversary = []\n",
    "    loss_training_classifier = []\n",
    "    correct = []\n",
    "    step = 0 \n",
    "    \n",
    "    #train classifier\n",
    "    for sub_X,sub_Y in data.generate_data():\n",
    "        \n",
    "        big_training = sub_X[3]\n",
    "        big_training_pivot = sub_X[0]\n",
    "        big_training_sv = sub_X[4]\n",
    "        big_target = sub_Y[0]\n",
    "\n",
    "        trainingv = Variable(torch.FloatTensor(big_training))\n",
    "        trainingv_pivot = Variable(torch.from_numpy(np.argmax(big_training_pivot, axis = 1)).long())\n",
    "        trainingv_sv = Variable(torch.FloatTensor(big_training_sv))\n",
    "        targetv = Variable(torch.from_numpy(np.argmax(big_target, axis = 1)).long()) \n",
    "        trainingv, valv = torch.split(trainingv, int(trainingv.size()[0] * (1 - val_split)))\n",
    "        trainingv_pivot, valv_pivot = torch.split(trainingv_pivot, int(trainingv_pivot.size()[0] * (1 - val_split)))\n",
    "        trainingv_sv, valv_sv = torch.split(trainingv_sv, int(trainingv_sv.size()[0] * (1 - val_split)))\n",
    "        targetv, val_targetv = torch.split(targetv, int(targetv.size()[0] * (1 - val_split)))\n",
    "        samples_random = np.random.choice(range(len(trainingv)), valv.size()[0]/100)\n",
    "        \n",
    "        print('\\nStep: ' + str(step) + '/' + str(file_number))\n",
    "        step += 1\n",
    "        \n",
    "        for j in range(0, trainingv.size()[0], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            out = gnn(trainingv[j:j + batch_size].cuda(), trainingv_sv[j:j + batch_size].cuda(), trainingv_pivot[j:j + batch_size].cuda())\n",
    "            out_DfR = DfR(trainingv[j:j + batch_size].cuda(), trainingv_sv[j:j + batch_size].cuda(), trainingv_pivot[j:j + batch_size].cuda())\n",
    "            l = loss(out[0], targetv[j:j + batch_size].cuda())\n",
    "            l_DfR = loss_R(out_DfR, trainingv_pivot[j:j + batch_size].cuda())\n",
    "            l = (c * l) - (lam * l_DfR)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            loss_string = \"Loss: %s\" % \"{0:.5f}\".format(l.item())\n",
    "            util.printProgressBar(j + batch_size, trainingv.size()[0], \n",
    "                                  prefix = \"%s [%s/%s] \" % (loss_string, \n",
    "                                                           j + batch_size, \n",
    "                                                           trainingv.size()[0]),\n",
    "                                  length = 20)\n",
    "    \n",
    "    \n",
    "    torch.save(gnn.state_dict(), 'IN_opendata_adversarial_classifier_lambda100_V2')\n",
    "    \n",
    "    #train adversary\n",
    "    step = 0 \n",
    "    \n",
    "    for sub_X,sub_Y in data.generate_data():\n",
    "        \n",
    "        big_training = sub_X[3]\n",
    "        big_training_pivot = sub_X[0]\n",
    "        big_training_sv = sub_X[4]\n",
    "        big_target = sub_Y[0]\n",
    "\n",
    "        trainingv = Variable(torch.FloatTensor(big_training))\n",
    "        trainingv_pivot = Variable(torch.from_numpy(np.argmax(big_training_pivot, axis = 1)).long())\n",
    "        trainingv_sv = Variable(torch.FloatTensor(big_training_sv))\n",
    "        targetv = Variable(torch.from_numpy(np.argmax(big_target, axis = 1)).long()) \n",
    "        trainingv, valv = torch.split(trainingv, int(trainingv.size()[0] * (1 - val_split)))\n",
    "        trainingv_pivot, valv_pivot = torch.split(trainingv_pivot, int(trainingv_pivot.size()[0] * (1 - val_split)))\n",
    "        trainingv_sv, valv_sv = torch.split(trainingv_sv, int(trainingv_sv.size()[0] * (1 - val_split)))\n",
    "        targetv, val_targetv = torch.split(targetv, int(targetv.size()[0] * (1 - val_split)))\n",
    "        samples_random = np.random.choice(range(len(trainingv)), valv.size()[0]/100)\n",
    "        \n",
    "        print('\\nStep: ' + str(step) + '/' + str(file_number))\n",
    "        step += 1\n",
    "\n",
    "        for j in range(0, trainingv.size()[0], batch_size):    \n",
    "            if lam > 0.0: \n",
    "                DfR = DfR.train()\n",
    "                gnn = gnn.eval()\n",
    "                opt_DfR.zero_grad()\n",
    "                out_DfR = DfR(trainingv[j:j + batch_size].cuda(), trainingv_sv[j:j + batch_size].cuda(), trainingv_pivot[j:j + batch_size].cuda())\n",
    "                l_DfR = c * loss_R(out_DfR, trainingv_pivot[j:j + batch_size].cuda())\n",
    "                l_DfR.backward()\n",
    "                opt_DfR.step()\n",
    "                loss_string = \"Loss: %s\" % \"{0:.5f}\".format(l_DfR.item())\n",
    "                util.printProgressBar(j + batch_size, trainingv.size()[0], \n",
    "                                      prefix = \"%s [%s/%s] \" % (loss_string, \n",
    "                                                               j + batch_size, \n",
    "                                                               trainingv.size()[0]),\n",
    "                                      length = 20)\n",
    "            \n",
    "    torch.save(DfR.state_dict(), 'IN_opendata_adversarial_adversary_lambda100_V2')\n",
    "    \n",
    "    gnn.eval()\n",
    "    DfR.eval()\n",
    "    \n",
    "    # Validation Loss\n",
    "    for sub_X,sub_Y in data.generate_data():\n",
    "        \n",
    "        big_training = sub_X[3]\n",
    "        big_training_pivot = sub_X[0]\n",
    "        big_training_sv = sub_X[4]\n",
    "        big_target = sub_Y[0]\n",
    "        \n",
    "\n",
    "        trainingv = Variable(torch.FloatTensor(big_training))\n",
    "        trainingv_pivot = Variable(torch.from_numpy(np.argmax(big_training_pivot, axis = 1)).long())\n",
    "        trainingv_sv = Variable(torch.FloatTensor(big_training_sv))\n",
    "        targetv = Variable(torch.from_numpy(np.argmax(big_target, axis = 1)).long()) \n",
    "        trainingv, valv = torch.split(trainingv, int(trainingv.size()[0] * (1 - val_split)))\n",
    "        trainingv_pivot, valv_pivot = torch.split(trainingv_pivot, int(trainingv_pivot.size()[0] * (1 - val_split)))\n",
    "        trainingv_sv, valv_sv = torch.split(trainingv_sv, int(trainingv_sv.size()[0] * (1 - val_split)))\n",
    "        targetv, val_targetv = torch.split(targetv, int(targetv.size()[0] * (1 - val_split)))\n",
    "        samples_random = np.random.choice(range(len(trainingv)), valv.size()[0]/100)\n",
    "        \n",
    "        \n",
    "        # Validation Loss\n",
    "\n",
    "        for j in range(0, valv.size()[0], batch_size):\n",
    "            out = gnn(valv[j:j + batch_size].cuda(), valv_sv[j:j + batch_size].cuda(), valv_pivot[j:j + batch_size].cuda())\n",
    "            out_DfR = DfR(valv[j:j + batch_size].cuda(), valv_sv[j:j + batch_size].cuda(), valv_pivot[j:j + batch_size].cuda())\n",
    "            l = loss(out[0], val_targetv[j:j + batch_size].cuda())\n",
    "            l_DfR = loss_R(out_DfR, valv_pivot[j:j + batch_size].cuda())\n",
    "            l_val = (c * l) - (lam * l_DfR)\n",
    "            lst.append(out[0].cpu().data.numpy())\n",
    "            loss_val_classifier.append(l.item())\n",
    "            loss_val_adversary.append(l_DfR.item())\n",
    "            \n",
    "            \n",
    "        val_targetv_cpu = val_targetv.cpu().data.numpy()\n",
    "        for n in range(val_targetv_cpu.shape[0]):\n",
    "            correct.append(val_targetv_cpu[n])\n",
    "\n",
    "        # Training Loss\n",
    "\n",
    "        for j in samples_random:\n",
    "            out = gnn(trainingv[j:j + batch_size].cuda(), trainingv_sv[j:j + batch_size].cuda(), trainingv_pivot[j:j + batch_size].cuda())\n",
    "            out_DfR = DfR(trainingv[j:j + batch_size].cuda(), trainingv_sv[j:j + batch_size].cuda(), trainingv_pivot[j:j + batch_size].cuda())\n",
    "            l = loss(out[0], targetv[j:j + batch_size].cuda())\n",
    "            l_DfR = loss_R(out_DfR, trainingv_pivot[j:j + batch_size].cuda())\n",
    "            l_training = (c * l) - (lam * l_DfR)\n",
    "            loss_training_classifier.append(l.item())\n",
    "            loss_training_adversary.append(l_DfR.item())\n",
    "            \n",
    "        \n",
    "    l_val_adversary = np.mean(np.array(loss_val_adversary))\n",
    "    l_val_classifier = np.mean(np.array(loss_val_classifier))\n",
    "    predicted = (torch.FloatTensor(np.concatenate(lst))).to(device)\n",
    "    print('\\nValidation Loss: ', l_val)\n",
    "\n",
    "    l_training_adversary = np.mean(np.array(loss_training_adversary))\n",
    "    l_training_classifier = np.mean(np.array(loss_training_classifier))\n",
    "    print('Training Loss: ', l_training)\n",
    "    val_targetv = torch.FloatTensor(np.array(correct)).cuda()\n",
    "   \n",
    "    torch.save(gnn.state_dict(), 'IN_opendata_adversarial_classifier_lambda100_V2')\n",
    "    acc_vals[final_epoch] = stats(predicted, val_targetv)\n",
    "    loss_vals_training_adversary[final_epoch] = l_training_adversary\n",
    "    loss_vals_validation_adversary[final_epoch] = l_val_adversary\n",
    "    loss_vals_training_classifier[final_epoch] = l_training_classifier\n",
    "    loss_vals_validation_classifier[final_epoch] = l_val_classifier\n",
    "    \n",
    "    if all(loss_vals_validation_classifier[max(0, m - 5):m] > min(np.append(loss_vals_validation_classifier[0:max(0, m - 5)], 200))) and m > 5:\n",
    "        if all(loss_vals_validation_adversary[max(0, m - 5):m] > min(np.append(loss_vals_validation_adversary[0:max(0, m - 5)], 200))) and m > 5:\n",
    "            print('Early Stopping...')\n",
    "            print(loss_vals_training, '\\n', np.diff(loss_vals_training))\n",
    "            break\n",
    "    \n",
    "    \n",
    "    print\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Loss Plot\n",
    "loss_vals_training = loss_vals_training_classifier[:(final_epoch)] \n",
    "loss_vals_validation = loss_vals_validation_classifier[:(final_epoch)] \n",
    "#loss_validation_std = loss_validation_std_classifier[:(final_epoch)] \n",
    "#loss_training_std = loss_training_std_classifier[:(final_epoch)] \n",
    "epochs = np.array(range(len(loss_vals_training)))\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(epochs, loss_vals_training, label='training')\n",
    "ax1.plot(epochs, loss_vals_validation, label='validation', color = 'green')\n",
    "#ax1.fill_between(epochs, loss_vals_validation - loss_validation_std/2, loss_vals_validation + loss_validation_std/2, color = 'lightgreen', label = 'Validation +/- 0.5 Std')\n",
    "#ax1.fill_between(epochs, loss_vals_training - loss_training_std/2, loss_vals_training + loss_training_std/2, color = 'lightblue', label = 'Training +/- 0.5 Std')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Loss Plot IN with Adversarial Training (Data Generator)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.savefig('Loss_SV_tracks_adversarial')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate ROC Plot\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "gnn.eval()\n",
    "prediction = np.array([])\n",
    "out = np.array([])\n",
    "for j in range(0, testv.size()[0], batch_size):\n",
    "    out_test = softmax(gnn(testv[j:j + batch_size].cuda(), testv_sv[j:j + batch_size].cuda(), testv_pivot[j:j + batch_size].cuda())[0])\n",
    "    out_test = out_test.cpu().data.numpy()\n",
    "   \n",
    "    for i in range(len(out_test)):\n",
    "        if (out_test[i][0] > out_test[i][1]): \n",
    "            prediction = np.append(prediction, out_test[i][0])\n",
    "            out = np.append(out, 0)\n",
    "        else: \n",
    "            prediction = np.append(prediction, out_test[i][1])\n",
    "            out = np.append(out, 1)\n",
    "\n",
    "for i in range(prediction.size): \n",
    "    if out[i] == 0: \n",
    "        prediction[i] = 1.0 - prediction[i]\n",
    "\n",
    "    \n",
    "fpr, tpr, thresholds = roc_curve(targetv_test.cpu().data.numpy(), prediction)\n",
    "auc = roc_auc_score(targetv_test.cpu().data.numpy(), prediction)\n",
    "\n",
    "fpr_DeepDoubleB = np.load('fpr_DeepDoubleB_LLF.npy')\n",
    "tpr_DeepDoubleB = np.load('tpr_DeepDoubleB_LLF.npy')\n",
    "dfpr_BDT = np.load('dfpr_BDT.npy')\n",
    "dtpr_BDT = np.load('dtpr_BDT.npy')\n",
    "tpr_IN = np.load('tpr_opendata_IN_Run2.npy')\n",
    "fpr_IN = np.load('fpr_opendata_IN_Run2.npy')\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "lw = 2\n",
    "plt.semilogy(tpr, fpr, color='darkorange',\n",
    "         lw=lw, label='IN with Adversarial (area = %0.2f)' % auc)\n",
    "plt.plot(tpr_IN, fpr_IN, color='red',\n",
    "         lw=lw, label='IN without Adversarial (area = 0.98)')\n",
    "plt.plot(tpr_DeepDoubleB, fpr_DeepDoubleB, color='blue',\n",
    "         lw=lw, label='DeepDoubleB without Adversarial (area = 0.97)')\n",
    "plt.plot(dtpr_BDT, dfpr_BDT, color='green',\n",
    "         lw=lw, label='BDT (area = 0.914)' % auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([10**-3, 1])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC IN with Adversarial Training (Data Generator)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.savefig('ROC_adversarial_Run6')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full = torch.FloatTensor(np.concatenate(np.array([test])))\n",
    "test_sv_full = torch.FloatTensor(np.concatenate(np.array([test_sv])))\n",
    "test_pivot_full = torch.FloatTensor(np.concatenate(np.array([test_pivot])))\n",
    "prediction_test = np.array([])\n",
    "gnn_out = np.array([])\n",
    "for j in range(0, test_full.size()[0], batch_size):\n",
    "    print(j)\n",
    "    out_test = softmax(gnn(test_full[j:j + batch_size].cuda(), test_sv_full[j:j + batch_size].cuda(), test_pivot_full[j:j + batch_size].cuda()))\n",
    "    out_test = out_test.cpu().data.numpy()\n",
    "    for i in range(len(out_test)):\n",
    "        if (out_test[i][0] > out_test[i][1]): \n",
    "            prediction_test = np.append(prediction_test, out_test[i][0])\n",
    "            gnn_out = np.append(gnn_out, 0)\n",
    "        else: \n",
    "            prediction_test = np.append(prediction_test, out_test[i][1])\n",
    "            gnn_out = np.append(gnn_out, 1)\n",
    "\n",
    "for i in range(prediction_test.size): \n",
    "    if gnn_out[i] == 0: \n",
    "        prediction_test[i] = 1.0 - prediction_test[i]\n",
    "    \n",
    "#np.save('out', out)\n",
    "#np.save('prediction', prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = torch.FloatTensor(np.concatenate(np.array([train])))\n",
    "train_sv_full = torch.FloatTensor(np.concatenate(np.array([train_sv])))\n",
    "prediction_train = np.array([])\n",
    "gnn_out = np.array([])\n",
    "for j in range(0, test_full.size()[0], batch_size * 10):\n",
    "    print(j)\n",
    "    out_test = softmax(gnn(test_full[j:j + batch_size * 10].cuda(), test_sv_full[j:j + batch_size * 10].cuda()))\n",
    "    out_test = out_test.cpu().data.numpy()\n",
    "    for i in range(len(out_test)):\n",
    "        if (out_test[i][0] > out_test[i][1]): \n",
    "            prediction_test = np.append(prediction_test, out_test[i][0])\n",
    "            gnn_out = np.append(gnn_out, 0)\n",
    "        else: \n",
    "            prediction_test = np.append(prediction_test, out_test[i][1])\n",
    "            gnn_out = np.append(gnn_out, 1)\n",
    "\n",
    "for i in range(prediction_test.size): \n",
    "    if gnn_out[i] == 0: \n",
    "        prediction_test[i] = 1.0 - prediction_test[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=1)\n",
    "test_full = torch.FloatTensor(np.concatenate(np.array([test])))\n",
    "test_sv_full = torch.FloatTensor(np.concatenate(np.array([test_sv])))\n",
    "test_pivot_full = torch.FloatTensor(np.concatenate(np.array([test_pivot])))\n",
    "prediction_test = np.array([])\n",
    "gnn_out = np.array([])\n",
    "IN_out = []\n",
    "for j in range(0, test_full.size()[0], batch_size):\n",
    "    print(j)\n",
    "    out_test = softmax(gnn(test_full[j:j + batch_size].cuda(), test_sv_full[j:j + batch_size].cuda(), test_pivot_full[j:j + batch_size].cuda())[0])\n",
    "    out_test = out_test.cpu().data.numpy()\n",
    "    for i in range(len(out_test)):\n",
    "        IN_out.append(out_test[i])\n",
    "    \n",
    "np.save('IN_out_pivot_lambda1000_V2', np.array(IN_out))\n",
    "#np.save('out_opendata', out)\n",
    "#np.save('prediction_opendata', prediction_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
