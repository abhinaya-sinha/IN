{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd.variable import *\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import util\n",
    "from __future__ import print_function\n",
    "import setGPU\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']=\"6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/nfshome/emoreno/IN/'\n",
    "#test_0 = np.load(save_path + 'test_features_0.npy')\n",
    "#test_0 = np.swapaxes(test_0, 1, 2)\n",
    "#test_1 = np.load(save_path + 'test_features_1.npy')\n",
    "#test_1 = np.swapaxes(test_1, 1, 2)\n",
    "test_2 = np.load(save_path + 'test_features_2.npy')\n",
    "test_2 = np.swapaxes(test_2, 1, 2)\n",
    "test_3 = np.load(save_path + 'test_features_3.npy')\n",
    "test_3 = np.swapaxes(test_3, 1, 2)\n",
    "target_test = np.load(save_path + 'test_truth_0.npy')\n",
    "params_0 = ['fj_jetNTracks',\n",
    "          'fj_nSV',\n",
    "          'fj_tau0_trackEtaRel_0',\n",
    "          'fj_tau0_trackEtaRel_1',\n",
    "          'fj_tau0_trackEtaRel_2',\n",
    "          'fj_tau1_trackEtaRel_0',\n",
    "          'fj_tau1_trackEtaRel_1',\n",
    "          'fj_tau1_trackEtaRel_2',\n",
    "          'fj_tau_flightDistance2dSig_0',\n",
    "          'fj_tau_flightDistance2dSig_1',\n",
    "          'fj_tau_vertexDeltaR_0',\n",
    "          'fj_tau_vertexEnergyRatio_0',\n",
    "          'fj_tau_vertexEnergyRatio_1',\n",
    "          'fj_tau_vertexMass_0',\n",
    "          'fj_tau_vertexMass_1',\n",
    "          'fj_trackSip2dSigAboveBottom_0',\n",
    "          'fj_trackSip2dSigAboveBottom_1',\n",
    "          'fj_trackSip2dSigAboveCharm_0',\n",
    "          'fj_trackSipdSig_0',\n",
    "          'fj_trackSipdSig_0_0',\n",
    "          'fj_trackSipdSig_0_1',\n",
    "          'fj_trackSipdSig_1',\n",
    "          'fj_trackSipdSig_1_0',\n",
    "          'fj_trackSipdSig_1_1',\n",
    "          'fj_trackSipdSig_2',\n",
    "          'fj_trackSipdSig_3',\n",
    "          'fj_z_ratio'\n",
    "          ]\n",
    "\n",
    "params_1 = ['pfcand_ptrel',\n",
    "          'pfcand_erel',\n",
    "          'pfcand_phirel',\n",
    "          'pfcand_etarel',\n",
    "          'pfcand_deltaR',\n",
    "          'pfcand_puppiw',\n",
    "          'pfcand_drminsv',\n",
    "          'pfcand_drsubjet1',\n",
    "          'pfcand_drsubjet2',\n",
    "          'pfcand_hcalFrac'\n",
    "         ]\n",
    "\n",
    "params_2 = ['track_ptrel',     \n",
    "          'track_erel',     \n",
    "          'track_phirel',     \n",
    "          'track_etarel',     \n",
    "          'track_deltaR',\n",
    "          'track_drminsv',     \n",
    "          'track_drsubjet1',     \n",
    "          'track_drsubjet2',\n",
    "          'track_dz',     \n",
    "          'track_dzsig',     \n",
    "          'track_dxy',     \n",
    "          'track_dxysig',     \n",
    "          'track_normchi2',     \n",
    "          'track_quality',     \n",
    "          'track_dptdpt',     \n",
    "          'track_detadeta',     \n",
    "          'track_dphidphi',     \n",
    "          'track_dxydxy',     \n",
    "          'track_dzdz',     \n",
    "          'track_dxydz',     \n",
    "          'track_dphidxy',     \n",
    "          'track_dlambdadz',     \n",
    "          'trackBTag_EtaRel',     \n",
    "          'trackBTag_PtRatio',     \n",
    "          'trackBTag_PParRatio',     \n",
    "          'trackBTag_Sip2dVal',     \n",
    "          'trackBTag_Sip2dSig',     \n",
    "          'trackBTag_Sip3dVal',     \n",
    "          'trackBTag_Sip3dSig',     \n",
    "          'trackBTag_JetDistVal'\n",
    "         ]\n",
    "\n",
    "params_3 = ['sv_ptrel',\n",
    "          'sv_erel',\n",
    "          'sv_phirel',\n",
    "          'sv_etarel',\n",
    "          'sv_deltaR',\n",
    "          'sv_pt',\n",
    "          'sv_mass',\n",
    "          'sv_ntracks',\n",
    "          'sv_normchi2',\n",
    "          'sv_dxy',\n",
    "          'sv_dxysig',\n",
    "          'sv_d3d',\n",
    "          'sv_d3dsig',\n",
    "          'sv_costhetasvpv'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import H5Data\n",
    "\n",
    "files = []\n",
    "for i in range(37):\n",
    "    files.append(\"/nfshome/emoreno/IN/data/train/data_\" + str(i))\n",
    "\n",
    "data = H5Data(batch_size = 100000,\n",
    "               cache = None,\n",
    "               preloading=0,\n",
    "               features_name='training_subgroup', labels_name='target_subgroup')\n",
    "data.set_file_names(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert two sets into two branch with one set in both and one set in only one (Use for this file)\n",
    "\n",
    "training = training_2\n",
    "test = test_2\n",
    "params = params_2\n",
    "training_sv = training_3\n",
    "test_sv = test_3\n",
    "params_sv = params_3\n",
    "N = test.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predict, target):\n",
    "    _, p_vals = torch.max(predict, 1)\n",
    "    r = torch.sum(target == p_vals.squeeze(1)).data.numpy()[0]\n",
    "    t = target.size()[0]\n",
    "    return r * 1.0 / t\n",
    "\n",
    "def stats(predict, target):\n",
    "    _, p_vals = torch.max(predict, 1)\n",
    "    t = target.cpu().data.numpy()\n",
    "    p_vals = p_vals.squeeze(0).cpu().data.numpy()\n",
    "    vals = np.unique(t)\n",
    "    for i in vals:\n",
    "        ind = np.where(t == i)\n",
    "        pv = p_vals[ind]\n",
    "        correct = sum(pv == t[ind])\n",
    "        print(\"  Target %s: %s/%s = %s%%\" % (i, correct, len(pv), correct * 100.0/len(pv)))\n",
    "    print(\"Overall: %s/%s = %s%%\" % (sum(p_vals == t), len(t), sum(p_vals == t) * 100.0/len(t)))\n",
    "    return sum(p_vals == t) * 100.0/len(t)\n",
    "\n",
    "NBINS=40 # number of bins for loss function\n",
    "MMAX = 200. # max value\n",
    "MMIN = 40. # min value\n",
    "LAMBDA = 0.30 # lambda for penalty\n",
    "\n",
    "def loss_kldiv(y_in,x):\n",
    "    \"\"\"\n",
    "    mass sculpting penlaty term usking kullback_leibler_divergence\n",
    "    y_in: truth [h, y]\n",
    "    x: predicted NN output for y\n",
    "    h: the truth mass histogram vector \"one-hot encoded\" (length NBINS=40)\n",
    "    y: the truth categorical labels  \"one-hot encoded\" (length NClasses=2)\n",
    "    \"\"\"\n",
    "    h = y_in[:,0:NBINS]\n",
    "    y = y_in[:,NBINS:NBINS+2]\n",
    "    h_all = K.dot(K.transpose(h), y)\n",
    "    h_all_q = h_all[:,0]\n",
    "    h_all_h = h_all[:,1]\n",
    "    h_all_q = h_all_q / K.sum(h_all_q,axis=0)\n",
    "    h_all_h = h_all_h / K.sum(h_all_h,axis=0)\n",
    "    h_btag_anti_q = K.dot(K.transpose(h), K.dot(tf.diag(y[:,0]),x))\n",
    "    h_btag_anti_h = K.dot(K.transpose(h), K.dot(tf.diag(y[:,1]),x))\n",
    "    h_btag_q = h_btag_anti_q[:,1]\n",
    "    h_btag_q = h_btag_q / K.sum(h_btag_q,axis=0)\n",
    "    h_anti_q = h_btag_anti_q[:,0]\n",
    "    h_anti_q = h_anti_q / K.sum(h_anti_q,axis=0)\n",
    "    h_btag_h = h_btag_anti_h[:,1]\n",
    "    h_btag_h = h_btag_h / K.sum(h_btag_h,axis=0)\n",
    "    h_anti_h = h_btag_anti_q[:,0]\n",
    "    h_anti_h = h_anti_h / K.sum(h_anti_h,axis=0)\n",
    "\n",
    "    return categorical_crossentropy(y, x) + \\\n",
    "        LAMBDA*kullback_leibler_divergence(h_btag_q, h_anti_q) + \\\n",
    "        LAMBDA*kullback_leibler_divergence(h_btag_h, h_anti_h)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "def get_cmap(n, name='hsv'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n + 1)\n",
    "\n",
    "def predicted_histogram(data, \n",
    "                        target, \n",
    "                        labels = None, \n",
    "                        nbins = 10, \n",
    "                        out = None,\n",
    "                        xlabel = None,\n",
    "                        title = None\n",
    "                       ):\n",
    "    \"\"\"@params:\n",
    "        data = n x 1 array of parameter values\n",
    "        target = n x categories array of predictions\n",
    "    \"\"\"\n",
    "    target = preprocessing.normalize(target, norm = \"l1\")\n",
    "    if labels == None:\n",
    "        labels = [\"\" for i in range(target.shape[1])]\n",
    "    #1 decide bins\n",
    "    ma = np.amax(data) * 1.0\n",
    "    mi = np.amin(data)\n",
    "    bins = np.linspace(mi, ma, nbins)\n",
    "    bin_size = bins[1] - bins[0]\n",
    "    bin_locs = np.digitize(data, bins, right = True)\n",
    "    #2 set up bin x category matrix\n",
    "    #  Each M(bin, category) = Sum over particles with param in bin of category\n",
    "    M = np.array([np.sum(target[np.where(bin_locs == i)], axis = 0) \n",
    "                  for i in range(nbins)])\n",
    "    #3 plot each category/bin\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "    bars = np.array([M[:, i] for i in range(M.shape[1])])\n",
    "    cmap = get_cmap(len(bars), 'viridis')\n",
    "    for i in range(len(bars)):\n",
    "        ax.bar(bins, bars[i], \n",
    "               bottom = sum(bars[:i]), \n",
    "               color = cmap(i), \n",
    "               label = labels[i],\n",
    "               width = bin_size\n",
    "              )\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "def generate_control_plots():\n",
    "    #global gnn\n",
    "    len_params = len(params)\n",
    "    path = '/nfshome/emoreno/IN/img/n-h-hb/'\n",
    "    #os.makedirs(path)\n",
    "    fr = 0\n",
    "    b = 1000\n",
    "    pred= None\n",
    "    while fr< valv.shape[0]: #beginning splitting up valv into batches because memory runs out\n",
    "        print (\"Predicting from\",fr)\n",
    "        valv_1 = valv[fr:fr+b,...]\n",
    "        p = gnn(valv_1.cuda())\n",
    "        valv_1.cpu()\n",
    "        p = p.cpu().data\n",
    "        fr +=b\n",
    "        if pred is None:\n",
    "            pred = p\n",
    "        else:\n",
    "            pred = np.append(pred,p,axis=0)\n",
    "        print (pred.shape) #end \n",
    "\n",
    "    d_target = np.array([util.get_list_from_num(i, length = n_targets) \n",
    "                             for i in val_targetv.cpu().data.numpy()])\n",
    "    p_target = pred#.cpu().data.numpy()\n",
    "    for i in range(len(params)):\n",
    "        xlabel = params[i]\n",
    "        labels = [\"None\", \"H\", \"H + b\"]\n",
    "        data = np.mean(valv.data.numpy()[:, i, :], axis = 1)\n",
    "        predicted_histogram(data, d_target, \n",
    "                            nbins = 50, labels = labels,\n",
    "                            xlabel = xlabel, \n",
    "                            title = \"Actual Distribution\"\n",
    "                           )\n",
    "        plt.savefig(path + xlabel + \"-actual.png\", dpi = 200)\n",
    "        predicted_histogram(data, p_target, \n",
    "                            nbins = 50, labels = labels,\n",
    "                            xlabel = xlabel,\n",
    "                            title = \"Predicted Distribution\"\n",
    "                           )\n",
    "        plt.savefig(path + xlabel + \"-predicted.png\", dpi = 200)\n",
    "        plt.close(\"all\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn import utils\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "class GraphNet(nn.Module):\n",
    "    def __init__(self, n_constituents, n_targets, params, hidden):\n",
    "        super(GraphNet, self).__init__()\n",
    "        self.hidden = hidden\n",
    "        self.P = len(params)\n",
    "        self.N = n_constituents\n",
    "        self.S = test_sv.shape[1]\n",
    "        self.Nv = test_sv.shape[2]\n",
    "        self.Nr = self.N * (self.N - 1)\n",
    "        self.Dr = 0\n",
    "        self.De = 5\n",
    "        self.Dx = 0\n",
    "        self.Do = 6\n",
    "        self.n_targets = n_targets\n",
    "        self.assign_matrices()\n",
    "        self.assign_matrices_SV()\n",
    "        #self.switch = switch\n",
    "        \n",
    "        self.Ra = torch.ones(self.Dr, self.Nr)\n",
    "        self.fr1 = nn.Linear(2 * self.P + self.Dr, hidden).cuda()\n",
    "        self.fr1_sv = nn.Linear(self.S + self.P + self.Dr, hidden).cuda()\n",
    "        self.fr2 = nn.Linear(hidden, hidden/2).cuda()\n",
    "        self.fr3 = nn.Linear(hidden/2, self.De).cuda()\n",
    "        self.fo1 = nn.Linear(self.P + self.Dx + (2 * self.De), hidden).cuda()\n",
    "        self.fo2 = nn.Linear(hidden, hidden/2).cuda()\n",
    "        self.fo3 = nn.Linear(hidden/2, self.Do).cuda()\n",
    "        self.fc1 = nn.Linear(self.Do * self.N, hidden).cuda()\n",
    "        self.fc2 = nn.Linear(hidden, hidden/2).cuda()\n",
    "        self.fc3 = nn.Linear(hidden/2, self.n_targets).cuda()\n",
    "        self.fc_fixed = nn.Linear(self.Do, self.n_targets).cuda()\n",
    "        #self.gru = nn.GRU(input_size = self.Do, hidden_size = 20, bidirectional = False).cuda()\n",
    "            \n",
    "    def assign_matrices(self):\n",
    "        self.Rr = torch.zeros(self.N, self.Nr)\n",
    "        self.Rs = torch.zeros(self.N, self.Nr)\n",
    "        receiver_sender_list = [i for i in itertools.product(range(self.N), range(self.N)) if i[0]!=i[1]]\n",
    "        for i, (r, s) in enumerate(receiver_sender_list):\n",
    "            self.Rr[r, i] = 1\n",
    "            self.Rs[s, i] = 1\n",
    "        self.Rr = (self.Rr).cuda()\n",
    "        self.Rs = (self.Rs).cuda()\n",
    "    \n",
    "    def assign_matrices_SV(self):\n",
    "        self.Rk = torch.zeros(self.N, self.Nr)\n",
    "        self.Rv = torch.zeros(self.Nv, self.Nr)\n",
    "        receiver_sender_list = [i for i in itertools.product(range(self.N), range(self.Nv)) if i[0]!=i[1]]\n",
    "        for i, (k, v) in enumerate(receiver_sender_list):\n",
    "            self.Rk[k, i] = 1\n",
    "            self.Rv[v, i] = 1\n",
    "        self.Rk = (self.Rk).cuda()\n",
    "        self.Rv = (self.Rv).cuda()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        ###PF Candidate - PF Candidate###\n",
    "        Orr = self.tmul(x, self.Rr)\n",
    "        Ors = self.tmul(x, self.Rs)\n",
    "        B = torch.cat([Orr, Ors], 1)\n",
    "        ### First MLP ###\n",
    "        B = torch.transpose(B, 1, 2).contiguous()\n",
    "        B = nn.functional.relu(self.fr1(B.view(-1, 2 * self.P + self.Dr)))\n",
    "        B = nn.functional.relu(self.fr2(B))\n",
    "        E = nn.functional.relu(self.fr3(B).view(-1, self.Nr, self.De))\n",
    "        del B\n",
    "        E = torch.transpose(E, 1, 2).contiguous()\n",
    "        Ebar = self.tmul(E, torch.transpose(self.Rr, 0, 1).contiguous())\n",
    "        del E\n",
    "        \n",
    "        ####Secondary Vertex - PF Candidate### \n",
    "        Ork = self.tmul(x, self.Rk)\n",
    "        Orv = self.tmul(y, self.Rv)\n",
    "        B = torch.cat([Ork, Orv], 1)\n",
    "        ### First MLP ###\n",
    "        B = torch.transpose(B, 1, 2).contiguous()\n",
    "        B = nn.functional.relu(self.fr1_sv(B.view(-1, self.S + self.P + self.Dr)))\n",
    "        B = nn.functional.relu(self.fr2(B))\n",
    "        E = nn.functional.relu(self.fr3(B).view(-1, self.Nr, self.De))\n",
    "        del B\n",
    "        E = torch.transpose(E, 1, 2).contiguous()\n",
    "        Ebar_sv = self.tmul(E, torch.transpose(self.Rr, 0, 1).contiguous())\n",
    "        del E\n",
    "\n",
    "        ####Final output matrix###\n",
    "        C = torch.cat([x, Ebar], 1)\n",
    "        del Ebar\n",
    "        C = torch.cat([C, Ebar_sv], 1)\n",
    "        del Ebar_sv\n",
    "        C = torch.transpose(C, 1, 2).contiguous()\n",
    "        ### Second MLP ###\n",
    "        C = nn.functional.relu(self.fo1(C.view(-1, self.P + self.Dx + (2 * self.De))))\n",
    "        C = nn.functional.relu(self.fo2(C))\n",
    "        O = nn.functional.relu(self.fo3(C).view(-1, self.N, self.Do))\n",
    "        #Taking the mean/sum of each column\n",
    "        #N = torch.mean(O, dim=1)\n",
    "        N = torch.sum(O, dim=1)\n",
    "        del C\n",
    "        ### Classification MLP ###\n",
    "        #N = nn.functional.relu(self.fc1(O.view(-1, self.Do * self.N)))\n",
    "        del O\n",
    "        #N = nn.functional.relu(self.fc2(N))\n",
    "        #N = nn.functional.relu(self.fc3(N))\n",
    "        N = nn.functional.relu(self.fc_fixed(N))\n",
    "        #P = np.array(N.data.cpu().numpy())\n",
    "        #N = np.zeros((128, 1, 6))\n",
    "        #for i in range(batch_size):\n",
    "        #    N[i] = np.array(np.split(P[i], self.Do))\n",
    "        #    N[1] = [P[i]]\n",
    "        #N, hn = self.gru(torch.tensor(N).cuda())\n",
    "        #print((N).shape)\n",
    "        return N \n",
    "            \n",
    "    def tmul(self, x, y):  #Takes (I * J * K)(K * L) -> I * J * L \n",
    "        x_shape = x.size()\n",
    "        y_shape = y.size()\n",
    "        return torch.mm(x.view(-1, x_shape[2]), y).view(-1, x_shape[1], y_shape[1])\n",
    "\n",
    "n_targets = test.shape[1]\n",
    "gnn = GraphNet(N, n_targets, params, 15)\n",
    "#gnn.load_state_dict(torch.load('gnn_SV_tracks_0.4.0.torch_dataGenerator'))\n",
    "\n",
    "def get_sample(training1, training2, target, choice):\n",
    "    target_vals = np.argmax(target, axis = 1)\n",
    "    ind, = np.where(target_vals == choice)\n",
    "    chosen_ind = np.random.choice(ind, 200000)\n",
    "    return training1[chosen_ind], training2[chosen_ind], target[chosen_ind]\n",
    "\n",
    "def get_sample_train(training1, training2, target, choice):\n",
    "    target_vals = np.argmax(target, axis = 1)\n",
    "    ind, = np.where(target_vals == choice)\n",
    "    chosen_ind = ind\n",
    "    #chosen_ind = np.random.choice(ind, 200000)\n",
    "    return training1[chosen_ind], training2[chosen_ind], target[chosen_ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test Set\n",
    "val_split = 0.1\n",
    "batch_size =128\n",
    "n_epochs = 100\n",
    "\n",
    "n_targets_test = target_test.shape[1]\n",
    "samples_test = [get_sample(test, test_sv, target_test, i) for i in range(n_targets_test)]\n",
    "tests = [i[0] for i in samples_test]\n",
    "tests_sv = [i[1] for i in samples_test]\n",
    "targets_tests = [i[2] for i in samples_test]\n",
    "big_test = np.concatenate(tests)\n",
    "big_test_sv = np.concatenate(tests_sv)\n",
    "big_target_test = np.concatenate(targets_tests)\n",
    "big_test, big_test_sv, big_target_test = utils.shuffle(big_test, big_test_sv, big_target_test)\n",
    "\n",
    "testv = (torch.FloatTensor(big_test)).cuda()\n",
    "testv_sv = (torch.FloatTensor(big_test_sv)).cuda()\n",
    "targetv_test = (torch.from_numpy(np.argmax(big_target_test, axis = 1)).long()).cuda()\n",
    "testv, valv_test = torch.split(testv, int(testv.size()[0] * (1 - val_split)))\n",
    "testv_sv, valv_test_sv = torch.split(testv_sv, int(testv_sv.size()[0] * (1 - val_split)))\n",
    "targetv_test, val_targetv_test = torch.split(targetv_test, int(targetv_test.size()[0] * (1 - val_split)))\n",
    "    \n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gnn.parameters(), lr = 0.0001)\n",
    "loss_vals_training = np.zeros(n_epochs)\n",
    "loss_validation_std = np.zeros(n_epochs)\n",
    "loss_training_std = np.zeros(n_epochs)\n",
    "loss_vals_validation = np.zeros(n_epochs)\n",
    "acc_vals = np.zeros(n_epochs)\n",
    "final_epoch = 0\n",
    "\n",
    "for m in range(n_epochs):\n",
    "    print(\"Epoch %s\" % m)\n",
    "    #torch.cuda.empty_cache()\n",
    "    final_epoch = m\n",
    "    lst = []\n",
    "    loss_val = []\n",
    "    loss_training = []\n",
    "    correct = []\n",
    "    \n",
    "    for sub_X,sub_Y in data.generate_data():\n",
    "        \n",
    "        training = sub_X[1]\n",
    "        training_sv = sub_X[2]\n",
    "        target = sub_Y[0]\n",
    "\n",
    "        # Training Set\n",
    "        n_targets = target.shape[1]\n",
    "        samples = [get_sample_train(training, training_sv, target, i) for i in range(n_targets)]\n",
    "        trainings = [i[0] for i in samples]\n",
    "        trainings_sv = [i[1] for i in samples]\n",
    "        targets = [i[2] for i in samples]\n",
    "        big_training = np.concatenate(trainings)\n",
    "        big_training_sv = np.concatenate(trainings_sv)\n",
    "        big_target = np.concatenate(targets)\n",
    "        big_training, big_training_sv, big_target = utils.shuffle(big_training, big_training_sv, big_target)\n",
    "\n",
    "        val_split = 0.1\n",
    "        batch_size =128\n",
    "        n_epochs = 100\n",
    "\n",
    "        trainingv = (torch.FloatTensor(big_training)).cuda()\n",
    "        trainingv_sv = (torch.FloatTensor(big_training_sv)).cuda()\n",
    "        targetv = (torch.from_numpy(np.argmax(big_target, axis = 1)).long()).cuda()\n",
    "        trainingv, valv = torch.split(trainingv, int(trainingv.size()[0] * (1 - val_split)))\n",
    "        trainingv_sv, valv_sv = torch.split(trainingv_sv, int(trainingv_sv.size()[0] * (1 - val_split)))\n",
    "        targetv, val_targetv = torch.split(targetv, int(targetv.size()[0] * (1 - val_split)))\n",
    "        samples_random = np.random.choice(range(len(trainingv)), valv.size()[0]/100)\n",
    "\n",
    "        for j in range(0, trainingv.size()[0], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            out = gnn(trainingv[j:j + batch_size].cuda(), trainingv_sv[j:j + batch_size].cuda())\n",
    "            l = loss(out, targetv[j:j + batch_size].cuda())\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            loss_string = \"Loss: %s\" % \"{0:.5f}\".format(l.item())\n",
    "            util.printProgressBar(j + batch_size, trainingv.size()[0], \n",
    "                                  prefix = \"%s [%s/%s] \" % (loss_string, \n",
    "                                                           j + batch_size, \n",
    "                                                           trainingv.size()[0]),\n",
    "                                  length = 20)\n",
    "        \n",
    "        del trainingv, training_sv, targetv, valv, valv_sv, val_targetv\n",
    "        \n",
    "    for sub_X,sub_Y in data.generate_data():\n",
    "        training = sub_X[1]\n",
    "        training_sv = sub_X[2]\n",
    "        target = sub_Y[0]\n",
    "\n",
    "        # Training Set\n",
    "        n_targets = target.shape[1]\n",
    "        samples = [get_sample_train(training, training_sv, target, i) for i in range(n_targets)]\n",
    "        trainings = [i[0] for i in samples]\n",
    "        trainings_sv = [i[1] for i in samples]\n",
    "        targets = [i[2] for i in samples]\n",
    "        big_training = np.concatenate(trainings)\n",
    "        big_training_sv = np.concatenate(trainings_sv)\n",
    "        big_target = np.concatenate(targets)\n",
    "        big_training, big_training_sv, big_target = utils.shuffle(big_training, big_training_sv, big_target)\n",
    "\n",
    "        val_split = 0.1\n",
    "        batch_size =128\n",
    "        n_epochs = 100\n",
    "\n",
    "        trainingv = (torch.FloatTensor(big_training)).cuda()\n",
    "        trainingv_sv = (torch.FloatTensor(big_training_sv)).cuda()\n",
    "        targetv = (torch.from_numpy(np.argmax(big_target, axis = 1)).long()).cuda()\n",
    "        trainingv, valv = torch.split(trainingv, int(trainingv.size()[0] * (1 - val_split)))\n",
    "        trainingv_sv, valv_sv = torch.split(trainingv_sv, int(trainingv_sv.size()[0] * (1 - val_split)))\n",
    "        targetv, val_targetv = torch.split(targetv, int(targetv.size()[0] * (1 - val_split)))\n",
    "        samples_random = np.random.choice(range(len(trainingv)), valv.size()[0]/100)\n",
    "        \n",
    "        # Validation Loss\n",
    "\n",
    "        for j in range(0, valv.size()[0], batch_size):\n",
    "            out = gnn(valv[j:j + batch_size].cuda(), valv_sv[j:j + batch_size].cuda())\n",
    "            lst.append(out.cpu().data.numpy())\n",
    "            l_val = loss(out, val_targetv[j:j + batch_size].cuda())\n",
    "            loss_val.append(l_val.item())\n",
    "\n",
    "        val_targetv_cpu = val_targetv.cpu().data.numpy()\n",
    "        for n in range(val_targetv_cpu.shape[0]):\n",
    "            correct.append(val_targetv_cpu[n])\n",
    "\n",
    "        # Training Loss\n",
    "\n",
    "        for j in samples_random:\n",
    "            out = gnn(trainingv[j:j + batch_size].cuda(), trainingv_sv[j:j + batch_size].cuda())\n",
    "            l_training = loss(out, targetv[j:j + batch_size].cuda())\n",
    "            loss_training.append(l_training.item())\n",
    "        \n",
    "        del trainingv, training_sv, targetv, valv, valv_sv, val_targetv\n",
    "\n",
    "    l_val = np.mean(np.array(loss_val))\n",
    "    predicted = (torch.FloatTensor(np.concatenate(lst))).to(device)\n",
    "    print('\\nValidation Loss: ', l_val)\n",
    "\n",
    "    l_training = np.mean(np.array(loss_training))\n",
    "    print('Training Loss: ', l_training)\n",
    "    val_targetv = torch.FloatTensor(np.array(correct)).cuda()\n",
    "    \n",
    "    torch.save(gnn.state_dict(), 'gnn_SV_tracks_0.4.0.torch_dataGenerator_3')\n",
    "    acc_vals[m] = stats(predicted, val_targetv)\n",
    "    loss_vals_training[m] = l_training\n",
    "    loss_vals_validation[m] = l_val\n",
    "    loss_validation_std[m] = np.std(np.array(loss_val))\n",
    "    loss_training_std[m] = np.std(np.array(loss_training))\n",
    "    if all(loss_vals_validation[max(0, m - 5):m] > min(np.append(loss_vals_validation[0:max(0, m - 5)], 200))) and m > 5:\n",
    "        print('Early Stopping...')\n",
    "        print(loss_vals_training, '\\n', np.diff(loss_vals_training))\n",
    "        break\n",
    "    print\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainingv, training_sv, targetv, valv, valv_sv, val_targetv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del testv, testv_sv, targetv_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gnn.state_dict(), 'gnn_SV_tracks_0.4.0.torch_dataGenerator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_control_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "path = '/nfshome/emoreno/IN/img/n-h-hb/'\n",
    "for xlabel in params:\n",
    "    display(Image(filename=path + xlabel + '-actual.png'))\n",
    "    display(Image(filename=path + xlabel + '-predicted.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Loss Plot\n",
    "loss_vals_training = loss_vals_training[:(final_epoch)] \n",
    "loss_vals_validation = loss_vals_validation[:(final_epoch)] \n",
    "loss_validation_std = loss_validation_std[:(final_epoch)] \n",
    "loss_training_std = loss_training_std[:(final_epoch)] \n",
    "epochs = np.array(range(len(loss_vals_training)))\n",
    "fig = plt.figure(figsize = (12,10))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(epochs, loss_vals_training, label='training')\n",
    "ax1.plot(epochs, loss_vals_validation, label='validation', color = 'green')\n",
    "ax1.fill_between(epochs, loss_vals_validation - loss_validation_std/2, loss_vals_validation + loss_validation_std/2, color = 'lightgreen', label = 'Validation +/- 0.5 Std')\n",
    "ax1.fill_between(epochs, loss_vals_training - loss_training_std/2, loss_vals_training + loss_training_std/2, color = 'lightblue', label = 'Training +/- 0.5 Std')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Loss Plot Plain IN (Data Generator)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.savefig('Loss_SV_tracks_data_generator')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12, 10), dpi = 200)\n",
    "plt.plot(acc_vals[:final_epoch])\n",
    "sns.set()\n",
    "plt.title('Accuracy Plain IN (Data Generator)')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.savefig(\"Accuracy_SV_tracks_dataGenerator\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate ROC Plot\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "gnn.eval()\n",
    "prediction = np.array([])\n",
    "out = np.array([])\n",
    "for j in range(0, testv.size()[0], batch_size):\n",
    "    out_test = softmax(gnn(testv[j:j + batch_size].cuda(), testv_sv[j:j + batch_size].cuda()))\n",
    "    out_test = out_test.cpu().data.numpy()\n",
    "   \n",
    "    for i in range(len(out_test)):\n",
    "        if (out_test[i][0] > out_test[i][1]): \n",
    "            prediction = np.append(prediction, out_test[i][0])\n",
    "            out = np.append(out, 0)\n",
    "        else: \n",
    "            prediction = np.append(prediction, out_test[i][1])\n",
    "            out = np.append(out, 1)\n",
    "\n",
    "for i in range(prediction.size): \n",
    "    if out[i] == 0: \n",
    "        prediction[i] = 1.0 - prediction[i]\n",
    "        \n",
    "fpr, tpr, thresholds = roc_curve(targetv_test.cpu().data.numpy(), prediction)\n",
    "auc = roc_auc_score(targetv_test.cpu().data.numpy(), prediction)\n",
    "\n",
    "fpr_DeepDoubleB = np.load('fpr_DeepDoubleB.npy')\n",
    "tpr_DeepDoubleB = np.load('tpr_DeepDoubleB.npy')\n",
    "dfpr_BDT = np.load('dfpr_BDT.npy')\n",
    "dtpr_BDT = np.load('dtpr_BDT.npy')\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "lw = 2\n",
    "plt.semilogy(tpr, fpr, color='darkorange',\n",
    "         lw=lw, label='Plain IN (area = %0.2f)' % auc)\n",
    "plt.plot(tpr_DeepDoubleB, fpr_DeepDoubleB, color='blue',\n",
    "         lw=lw, label='Plain DeepDoubleB (area = 0.97)')\n",
    "plt.plot(dtpr_BDT, dfpr_BDT, color='green',\n",
    "         lw=lw, label='BDT (area = 0.914)' % auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([10**-3, 1])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Plain IN (Data Generator)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_curve_data_generator')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full = torch.FloatTensor(np.concatenate(np.array([test])))\n",
    "test_sv_full = torch.FloatTensor(np.concatenate(np.array([test_sv])))\n",
    "prediction_test = np.array([])\n",
    "gnn_out = np.array([])\n",
    "for j in range(0, test_full.size()[0], batch_size):\n",
    "    print(j)\n",
    "    out_test = softmax(gnn(test_full[j:j + batch_size].cuda(), test_sv_full[j:j + batch_size].cuda()))\n",
    "    out_test = out_test.cpu().data.numpy()\n",
    "    for i in range(len(out_test)):\n",
    "        if (out_test[i][0] > out_test[i][1]): \n",
    "            prediction_test = np.append(prediction_test, out_test[i][0])\n",
    "            gnn_out = np.append(gnn_out, 0)\n",
    "        else: \n",
    "            prediction_test = np.append(prediction_test, out_test[i][1])\n",
    "            gnn_out = np.append(gnn_out, 1)\n",
    "\n",
    "for i in range(prediction_test.size): \n",
    "    if gnn_out[i] == 0: \n",
    "        prediction_test[i] = 1.0 - prediction_test[i]\n",
    "    \n",
    "#np.save('out', out)\n",
    "#np.save('prediction', prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = torch.FloatTensor(np.concatenate(np.array([train])))\n",
    "train_sv_full = torch.FloatTensor(np.concatenate(np.array([train_sv])))\n",
    "prediction_train = np.array([])\n",
    "gnn_out = np.array([])\n",
    "for j in range(0, test_full.size()[0], batch_size * 10):\n",
    "    print(j)\n",
    "    out_test = softmax(gnn(test_full[j:j + batch_size * 10].cuda(), test_sv_full[j:j + batch_size * 10].cuda()))\n",
    "    out_test = out_test.cpu().data.numpy()\n",
    "    for i in range(len(out_test)):\n",
    "        if (out_test[i][0] > out_test[i][1]): \n",
    "            prediction_test = np.append(prediction_test, out_test[i][0])\n",
    "            gnn_out = np.append(gnn_out, 0)\n",
    "        else: \n",
    "            prediction_test = np.append(prediction_test, out_test[i][1])\n",
    "            gnn_out = np.append(gnn_out, 1)\n",
    "\n",
    "for i in range(prediction_test.size): \n",
    "    if gnn_out[i] == 0: \n",
    "        prediction_test[i] = 1.0 - prediction_test[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('out', gnn_out)\n",
    "np.save('prediction', prediction_test)\n",
    "np.save('tpr', tpr)\n",
    "np.save('fpr', fpr)\n",
    "np.save('thresholds', thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
